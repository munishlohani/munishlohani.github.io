<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>A California House Price Predictor!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*psi79DaNUmdmU4__"><figcaption>Photo by <a href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Markus Spiske</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>Project One: California Housing Prices Dataset</h3> <h4><strong>Introduction</strong></h4> <p>It is finally time to code! For my first project, I followed the book “Hands-On Machine Learning with Scikit-Learn and TensorFlow.” The dataset used here gives an idea about housing prices based on location, rooms, bedrooms, etc. In this project, I will perform a basic machine learning task. To begin with, let’s complete some tasks.</p> <h4><strong>The Big Picture</strong></h4> <p>Our dataset contains data metrics such as population, median income, median housing cost, etc., for different districts.</p> <h4><strong>Frame The Problem</strong></h4> <blockquote><strong>Pipelines</strong></blockquote> <blockquote>In simple words, a data pipeline is a sequence of data processing components. Pipelines are crucial in machine learning, especially when managing complex workflows. This involves steps designed to transform our raw data into a model that can solve problems.</blockquote> <p>The next step is to ensure whether there are any existing ways to solve this problem. Knowing this will give us performance insights on how to solve the problem. For this problem, let’s assume that previously, the work was done manually: a separate department for data collection, a separate one for complex calculations, etc.</p> <p>Based on the current solution, we get an idea that the error in prediction can be high. What if we don’t have price data for some regions, and based on our complex calculations, we must predict a certain price? But then, what if the actual price comes out 10% or 20% off? Well, it certainly affects our business. But with our model, we can predict prices based on various features. Now that we know what our problem is, let’s frame it:</p> <ul> <li>Our Data is labelled- <strong>Supervised Learning</strong>.</li> <li>We are predicting the price- It is a regression task, specifically <strong>multiple regression</strong>.</li> <li>We do not have a continuous source of data and we do not need our model to adapt to constantly changing data- it is <strong>batch learning</strong>.</li> </ul> <h4>Select Performance Measure</h4> <p>Now that we have framed the problem, let’s select a performance measure for our model. In most regression tasks, we use <strong>Root Mean Square Error (RMSE)</strong> to measure our model’s performance.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/553/1*C-Yz2i_a1C4-OnVkbj6ZXA.png"><figcaption>RMSE</figcaption></figure> <p>Another performance measure is <strong>Mean Absolute Error.</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/223/1*2UxOtkGvNy8LzhC-hk-7JA.png"><figcaption>Mean Absolute Error</figcaption></figure> <p>Both RMSE and MAE are ways to calculate distance between the actual value vector and the predicted value vector.</p> <p><strong>RMSE</strong></p> <p>RMSE is based on Euclidean Norm. This measures the “straight-line” distance between two points in the Euclidean Space.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/292/1*7AuGH-ojQorMTxQWhljP-Q.png"><figcaption>Euclidean Norm</figcaption></figure> <p><strong>MAE</strong></p> <p>MAE is based on Manhattan Norm. This measures the average absolute distances between actual values and the predicted values.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/207/1*qMhvexxjK9FFK_ua_FdB4Q.png"><figcaption>Manhattan Norm</figcaption></figure> <p><strong>Check Assumptions</strong></p> <p>Finally, it is good practice to check whether the model that our data gets fit into categorizes the prices into categories or simply uses the prices themselves. For example, if the model uses categories, our task can be taken as a classification project instead of regression — we should be categorizing the house (cheap, medium, expensive) rather than predicting the price. For this project, let’s say it is a regression task.</p> <p>For this project, let’s say it is a regression task.</p> <h4>The Project</h4> <p>first, let’s load our data set.</p> <pre>import pandas as pd<br>import numpy as np<br><br>housing=pd.read_csv("E://books to read//100 Days of Machine Learning//Code//Project_1_Cali_House_Dataset//housing.csv")<br>housing.head()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yn5qYj8J5sN9WK4YNy2sbQ.png"><figcaption>Output</figcaption></figure> <pre>housing.info()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/530/1*bw51p2qxnPTzC6vAo51bIw.png"><figcaption>Output</figcaption></figure> <p>We see that total_bedrooms has some missing values. We will resolve this issue later. Here, we also find that every other feature is a number (float64) but ocean_proximity is an object.</p> <pre>housing["ocean_proximity"].value_counts()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/313/1*EncIlOVvCDTNmolBlK8feA.png"><figcaption>Output</figcaption></figure> <p>So, it seems that there are five categories of ocean_proximity. Since our ML model only understands numbers, we will later be converting it into one via OneHotEncoding. For now, let's visualize the data in the form of a histogram.</p> <p>For now, let’s visualize the data inform of a histogram.</p> <pre>import matplotlib.pyplot as plt<br>%matplotlib inline<br><br>housing.hist(bins=50,figsize=(20,15))<br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*B5MeG3iBC5qouYbdMsQvCw.png"><figcaption>Output</figcaption></figure> <p>From the figure, we can infer that most of the data seem to be negatively tailed. This might lead to a problem later in our ML model. So, it’s better to transform it into a bell-shaped curve. The next thing is the scale. For example, it seems that median_income has been transformed.</p> <p>Now, let's work on our data. In order to train our model, it is necessary to divide it into a training set and a testing set. A general rule is to allow 20% of data as testing and the rest as training. But, before we proceed, let's work on median_income. We see that most of the incomes are clustered around 1.5 and 6. But, there are also some beyond 6. So, it's important for us to have enough instances for each stratum.</p> <pre>housing["income_cat"]=pd.cut(housing['median_income'],bins=[0.,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])<br>housing["income_cat"].hist()<br>plt.plot()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/561/1*rd1688oCkulLAKGwC3e3Jw.png"><figcaption>output</figcaption></figure> <p>Now, lets work on training and testing set.</p> <pre>from sklearn.model_selection import StratifiedShuffleSplit<br><br>split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)<br>for train_index,test_index in split.split(housing,housing['income_cat']):<br>    strat_train=housing.loc[train_index]<br>    strat_test=housing.loc[test_index]</pre> <p>In order for the model to be effective and unbiased, it is necessary to have a good proportion of every category in our training set. While we could use sklearn's train_test_split to divide our data, a more effective way to divide it as per the income category is StratifiedShuffleSplit. Finally, let's get the data back into its original form.</p> <pre>for set_ in (strat_test,strat_train):<br>    set_.drop("income_cat",axis=1,inplace=True)</pre> <p>Until now, we’ve just went through the data without digging deep into it. But now, let’s explore the data further. One way of doing so is via visualizing.</p> <pre>copy_housing=strat_train.copy()<br>copy_housing.plot(kind="scatter",x="longitude",y="latitude",alpha=0.1)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/573/1*UODEQZxTelhuR7pkXWMJqQ.png"><figcaption>Output</figcaption></figure> <p>Well, this definately looks like California with higher density zones. Now, let’s visualize the income.</p> <pre>copy_housing.plot(kind="scatter",x="latitude",y="longitude",s=copy_housing["population"]/100,c="median_income",cmap=plt.get_cmap("jet"),alpha=0.3)<br>plt.legend()<br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/574/1*fgDp9BnsKSYkCL_UNmHhyg.png"><figcaption>Output</figcaption></figure> <p>Well, another way of exploring data is through finding correlation.</p> <pre>#Corr<br>corr=copy_housing.drop("ocean_proximity",axis="columns").corr()<br><br>from pandas.plotting import scatter_matrix<br><br>attributes=['median_house_value','median_income','total_rooms','housing_median_age']<br>scatter_matrix(copy_housing[attributes],figsize=(22,18))</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uEfVR0zjNekIFJioDq3vTQ.png"><figcaption>Output</figcaption></figure> <p>From this graph, we can find that median_income and median_house_value have a quite linear relationship, whereas others aren't so clear. Well now, let's experiment by joining some values.</p> <pre>copy_housing["rooms_per_household"]=copy_housing["total_rooms"]/copy_housing["households"]<br>copy_housing["bedrooms_per_room"]=copy_housing["total_bedrooms"]/copy_housing["total_rooms"]<br>copy_housing["population_per_household"]=copy_housing["population"]/copy_housing["households"]<br><br>corr=copy_housing.drop("ocean_proximity",axis="columns").corr()<br>corr["median_house_value"].sort_values(ascending=False)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/487/1*HHJu2_vjQWteMsXVoL7lIw.png"><figcaption>Output</figcaption></figure> <p>Well, joining the dataset indeed helped us as rooms_per_household has a high correlation value with median_house_value. Now that we have performed some feature engineering, let's prepare our dataset based on features and labels.</p> <pre>housing_features=strat_train.drop("median_house_value",axis=1)<br>housing_label=strat_train['median_house_value'].copy()</pre> <p>Now, we have perhaps reached an important point in our project. It’s time to create a pipeline. Remember, we had certain data missing in total_bedrooms? Now, we will be fixing it via sklearn's SimpleImputer.</p> <p>A SimpleImputer is used in the preprocessing stage of data preparation to handle missing values in a dataset. The goal is to replace null or missing values so that they do not negatively affect the performance of machine learning models. SimpleImputer does this by imputing values based on one of several strategies: Mean, Median, High Frequency, or Constant. Next, we will also be fixing the problem of tail-heavy data via StandardScaler.</p> <pre>from sklearn.preprocessing import StandardScaler<br>from sklearn.pipeline import Pipeline<br>from sklearn.impute import SimpleImputer<br><br>pipeline=Pipeline([<br>    ("imputer",SimpleImputer(strategy="median")),<br>    ("std_Scaler",StandardScaler())<br>])</pre> <p>Now that we’ve created a pipeline to fix our data issues, let’s create a ColumnTransformer to specify the transformation for different columns.</p> <pre>num_features=housing_features.drop("ocean_proximity",axis=1)<br>num_attrs=list(num_features)<br>cat_attrs=["ocean_proximity"]<br>full_pipeline=ColumnTransformer([<br>    ("num",pipeline,num_attrs),<br>    ("cat",OneHotEncoder(),cat_attrs)<br>])<br><br>housing_prepares=full_pipeline.fit_transform(housing_features)</pre> <p>This concludes our preprocessing. Firstly, we fixed the issue of tail-heavy data &amp; NaN data. Then, we converted categorical data into numerical data.</p> <p>Now, we will start building our models.</p> <p><strong>Linear Regression</strong></p> <pre>from sklearn.linear_model import LinearRegression<br><br>lin_reg=LinearRegression()<br>lin_reg.fit(housing_prepares,housing_label)</pre> <pre>from sklearn.metrics import mean_squared_error<br><br>predictions=lin_reg.predict(housing_prepares)<br>lin_mse=mean_squared_error(housing_label,predictions)<br>lin_rmse=np.sqrt(lin_mse)<br>lin_rmse</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/205/1*lsYpJgTFLTyx5o4CPzooOQ.png"><figcaption>RMSE output</figcaption></figure> <p>The value is better than nothing, but it is not perfect- in fact quite error prone. Most of our housing prices ranges from $128,000 to $225,000. So, an error of around $68,000 is rather imperfect. This means, our model isn’t powerful enough to grasp the information from our features.</p> <p><strong>Decision Tree</strong></p> <pre>from sklearn.tree import DecisionTreeRegressor<br>tree_reg=DecisionTreeRegressor()<br>tree_reg.fit(housing_prepares,housing_label)</pre> <pre>tree_predictions=tree_reg.predict(housing_prepares)<br>tree_mse=mean_squared_error(housing_label,tree_predictions)<br>tree_rmse=np.sqrt(tree_mse)<br><br>tree_rmse<br><br>#Output:0.0<br></pre> <p>Well, our model turns out to be error free. But here is a problem- it shouldn't actually be perfect. By this, we can imply that the model is overfitting. So, how do we solve it? One way is to use sklearn’s cross_val_score.</p> <p>cross_val_score divides the training set into K-subsets, called folds. It involves training the model on some data set and then validating it on the remaining subset.</p> <pre>from sklearn.model_selection import cross_val_score<br><br>cross_val_scores=cross_val_score(tree_reg,housing_prepares,housing_label,scoring="neg_mean_squared_error",cv=10)<br>tree_rmse_scores=np.sqrt(-cross_val_scores)<br><br>tree_rmse_scores</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/758/1*sWp2Vqa9Q7Q5lHiynMnbUg.png"><figcaption>Output</figcaption></figure> <p>Then, we can aggregate the mean value for all these values using np.mean() which comes out 71857.76227885179.</p> <p>Well, the Decision Tree performs worse as compared to Linear Regression.</p> <p><strong>Random Forest Regressor</strong></p> <p>Finally, let’s use RandomForestRegressor. A RandomForestRegressor is an ensemble learning method that constructs numerous DecisionTrees during training and outputs mean result.</p> <pre>from sklearn.ensemble import RandomForestRegressor<br><br>forest_reg=RandomForestRegressor()<br>forest_reg.fit(housing_prepares,housing_label)<br>forest_pred=forest_reg.predict(housing_prepares)<br>forest_mse=mean_squared_error(housing_label,forest_pred)<br>forest_rmse=np.sqrt(forest_mse)<br>forest_rmse<br><br>#Output: 18694.91813388159</pre> <pre>forest_cross_val_scores=cross_val_score(forest_reg,housing_prepares,housing_label,scoring="neg_mean_squared_error",cv=10)<br>forest_rmse_scores=np.sqrt(-cross_val_scores)<br><br>forest_rmse_scores<br><br>#Output: 68321.7118618  </pre> <p>Well, RandomForest looks much more promising, but the model is still overfitting. One way to solve this problem is by fiddling with hyperparameters.</p> <p><strong>GridSearchCV</strong></p> <p>sklearn’s GridSearchCV can help us automate the process of hyperparameter tuning. It also evaluates the performance of our models using cross validation.</p> <pre>from sklearn.model_selection import GridSearchCV<br><br>param_grid=[{"n_estimators":[3,10,30,60],"max_features":[2,4,6,8,10]},<br>{"bootstrap":[False],"n_estimators":[3,10],<br> "max_features":[2,3,4]}]<br><br>forest_reg=RandomForestRegressor()<br>grid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring="neg_mean_squared_error",return_train_score=True)<br><br>grid_search.fit(housing_prepares,housing_label)</pre> <pre>grid_search.best_params_<br><br>#Output: {'max_features': 6, 'n_estimators': 60}</pre> <p>The .best_params_ gives the most effective hyperparameters for our model.</p> <pre>final_model=grid_search.best_estimator_<br><br>x_test=strat_test.drop("median_house_value",axis=1)<br>y_test=strat_test["median_house_value"].copy()<br><br>x_test["rooms_per_household"]=x_test["total_rooms"]/x_test["households"]<br>x_test["bedrooms_per_room"]=x_test["total_bedrooms"]/x_test["total_rooms"]<br>x_test["population_per_household"]=x_test["population"]/x_test["households"]<br><br>x_test_prepared=full_pipeline.transform(x_test)<br><br>final_pred=final_model.predict(x_test_prepared)</pre> <pre>final_mse=mean_squared_error(y_test,final_pred)<br>final_rmse=np.sqrt(final_mse)<br><br>final_rmse<br><br>#Output: 47297.496275008285</pre> <p>The root-mean square for our final model is the best one among the model we tried earlier. But our task doesn't end here. In some cases, we might have to find the range of our generalized error to be convinced. This can be done by finding the confidence interval for RMSE.</p> <pre>from scipy import stats<br><br>confidence=0.95<br>error=(final_pred-y_test)**2<br>ci=np.sqrt(stats.t.interval(confidence,len(error)-1,loc=error.mean(),scale=stats.sem(error)))<br><br>interval=ci[1]-ci[0]<br>mean_rmse=(ci[0]+ci[1])/2<br>proportion=(ci[1]-ci[0])/mean_rmse<br><br>print(ci)<br><br><br>#Output:[45333.94519671 49182.71770318]<br></pre> <h4>Reflection</h4> <p>By the end of day 6, not only was I able to learn the concepts of many Machine Learning Techniques but was also able to implement it.</p> <p>While this project is based in the book itself, I will also be working on a separate project to avoid “tutorial hell”.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a77b9b0ed97e" width="1" height="1" alt=""></p> </body></html>