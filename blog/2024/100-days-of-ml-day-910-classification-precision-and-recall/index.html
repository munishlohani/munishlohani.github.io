<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>A deep dive into performance metrics- Precision, Recall, and F-1 Score!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JWqaam7vreotjAQM"><figcaption>Photo by <a href="https://unsplash.com/@dawson2406?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Stephen Dawson</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>Previously, we looked at confusion matrix and acuracy. Now, we will be looking at 3 other metrices that can be used to evaluate a classification model.</p> <h4>Introduction</h4> <p>Precision and Recall are two crucial metrics used in model evaluation in classification tasks especially when dealing with imbalanced datasets.</p> <h4>Precision</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/322/1*MsxGp4AV_ENOYEDi2diw6g.png"><figcaption>Precision</figcaption></figure> <p>Precision, also known as Positive Predictive Value, measures the proportion of True Positives (TP) out of all positive predictions (TP + FP) made by the model. Higher precision indicates better model performance with a low false positive rate.</p> <h4>Recall</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/218/1*s1sdVlXYggchN4eZswXV5g.png"><figcaption>Recall</figcaption></figure> <p>Recall, also known as Sensitivity, measures the proportion of True Positive (TP) predictions out of all actual positive instances. Higher recall indicates a lower false negative rate.</p> <p>Now that we have a basic idea of Precision and recall, let’s use them in our code.</p> <pre>from sklearn.metrics import precision_score,recall_score<br><br>precision=precision_score(y_train_5,y_train_pred)<br>recall=recall_score(y_train_5,y_train_pred)<br><br><br><br>print(f'''<br>      Precision:{precision}<br>      Recall:{recall}<br>      <br>      ''')<br><br>#Output:  Precision:0.7547327860613168<br> #       Recall:0.8310274857037447</pre> <p>Now, our model doesnt look as attractive as it did earler with accuracy.</p> <p>With 0.754 precision, it means that 75.4% of`5 predictions were true. Lokewise, Recall of 0.831 means that out of all the actual positive values, the model predicted 83.1% correctly.</p> <h4>What if?</h4> <p><strong>High Precision, low Recall</strong></p> <p>High Precision, low Recall happens when the model is too conservative in predicting the positive class. This means the proportion of false positives is lower than that of false negatives.</p> <p><strong>Low Precision, High Recall</strong></p> <p>Low Precision and High Recall happen when the model is too aggressive in predicting positive classes. This means, the model tries to capture all positive cases, which minimizes false negatives and increases false positives.</p> <h4>F1 Score</h4> <p>It is often convenient to combine precision and recall into a single metric. We call it F1 Score. The F1 Score is the <em>Harmonic Mean</em><strong><em> </em></strong>of precision and recall, providing a balance between the two.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/330/1*6xJDSHv1gRPQAz4mK7e5bw.png"><figcaption>Harmonic Mean, F1 Score</figcaption></figure> <p>Unlike a regular mean that treats all value equally, the harmonic mean gives much more weight to low values. As a result, to obtain a high F1 Score, it is necessary for both recall and precision to be high.</p> <pre>from sklearn.metrics import f1_score<br><br>f1_score=f1_score(y_train_5,y_train_pred)<br><br>print(f1_score)<br><br>#Output:  0.7910447761194029</pre> <p>The F1 score favors classifiers with balanced precision and recall. However, the desired metric depends on the specific case. For instance, for a kid-safe video classifier, high precision is preferred to avoid false positives, while for a fraud detection model, high recall is crucial to minimize false negatives.</p> <p>But the problem is we can’t have it both ways: increasing precision reduces recall and vice-versa.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=27ace9b94dd1" width="1" height="1" alt=""></p> </body></html>