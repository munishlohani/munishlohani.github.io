<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>A deep dive into more classification metrics: Precision/ Recall Tradeoff and ROC Curve!</p> <h3>Introduction</h3> <p>Previously, we discussed Precision, Recall, and F-1 Score. This blog continues from my previous one. If you haven’t checked it out yet, you can do so <a href="https://medium.com/@munish.lohani/100-days-of-ml-day-9-10-classification-precision-and-recall-27ace9b94dd1" rel="external nofollow noopener" target="_blank">here</a>. Now, let’s get into our main topic!</p> <h3>Precision/Recall Tradeoff</h3> <p>In classification, we cannot have both Precision and Recall equally high. Improving one decreases the other, known as the Precision/Recall Tradeoff. To understand this, let’s go back to our SGDClassifier. For each instance, our model computes a score based on its decision function. For now, let’s say if the score is greater than its threshold, the result is positive; if it’s less, negative.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qjq57VBNs4woHWlPgMEACg.png"><figcaption>Precision/Recall Tradeoff</figcaption></figure> <p>For this example, suppose the threshold is in the center. Now, based on this information, let’s calculate our Precision and Recall. Out of 5 positive predictions, 4 of them successfully identified the digit as 5, whereas 1 prediction was wrong. This gives:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/305/1*w1MBQ5Oz0pEmj3_eDw7-Xw.png"><figcaption>Precision and Recall Calculation</figcaption></figure> <p>Now, let’s slightly move our threshold to the left. This gives,</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fqL-wTkAesn0KKOQCYYt-w.png"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/336/1*MyZO_dQ6flym4qJUowfG1g.png"></figure> <p>As we can see, the Precision went down whereas we have a perfect Recall.</p> <p>Scikit-learn doesn’t let us set the threshold directly but it does allow us to access the decision scores using decision_function</p> <pre>y_scores=sgd_clf.decision_function(X[0].reshape(1,-1))<br>threshold=0<br>y_score_predict=(y_scores&gt;threshold)<br>y_score_predict<br><br><br># Output: array([ True])</pre> <pre>threshold=6700<br>y_score_predict=(y_scores&gt;threshold)<br>y_score_predict<br><br><br># Output: array([False])</pre> <p>This shows that by changing the threshold, we decreased the Recall. But how do we know which threshold to use? For this, we can find out the decision scores of all the instances and then compute Precision &amp; Recall for all possible thresholds.</p> <pre>y_scores=cross_val_predict(sgd_clf,X_train,y_train_5,cv=5,method='decision_function')<br><br>from sklearn.metrics import precision_recall_curve<br><br>precision,recall,threshold=precision_recall_curve(y_train_5,y_scores)</pre> <pre>def plot_prec_recall_thrshld(precision,recall,threshold):<br>    plt.plot(threshold,precision[:-1],"b--",label="Precision")<br>    plt.plot(threshold,recall[:-1],"r",label="Recall")<br>    plt.legend()<br>    plt.xlabel("Threshold")<br>    plt.grid(visible=True)<br><br><br>plot_prec_recall_thrshld(precision,recall,threshold)<br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*Yxfk6Q57bhArMLh5q9S95g.png"><figcaption>Precision and Recall Comparision</figcaption></figure> <p>Another way to visualize this is by plotting Precision vs. Recall.</p> <pre>plt.plot(recall,precision,"b-",label="Precision")<br>plt.xlabel("precision")<br>plt.ylabel("Recall")<br>plt.grid(visible=True)<br>plt.show()<br><br></pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*_K6_Rrur1pIx_77ik-KWKQ.png"><figcaption>Precision Vs Recall</figcaption></figure> <h3>The ROC Curve</h3> <p>The Receiver Operating Characteristic Curve is another method used in binary classifiers. Instead of plotting the Precision vs. Recall curve, the ROC curve plots the True Positive Rate (another name for Recall) vs. False Positive Rate (1 — True Negative Rate). To understand better, let’s take the above example.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qjq57VBNs4woHWlPgMEACg.png"></figure> <p>Here,</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/452/1*1qYhhcafMoMU4-VK-JFiJA.png"></figure> <h4><strong>AUC</strong></h4> <p>The Area Under the ROC curve is a single scalar value that summarizes the classifier's overall performance. Ranging from 0–1, the higher the value, the better performing the model. Now that we know what a ROC curve is, let’s use it for our model.</p> <pre>from sklearn.metrics import roc_auc_score<br><br>roc_auc_score(y_train_5,y_scores)<br><br><br># Output: 0.9648211175804801</pre> <p>As a rule of thumb, we use the P-R curve whenever the positive class is rare or when we care more about False Positives. In our case, since the positive class (5) occurs much more rarely than the negative class (not 5), the PR curve is better suited.</p> <p>With this, let’s try this on a different model — a RandomForest!</p> <pre>from sklearn.ensemble import RandomForestClassifier<br><br><br>forest_clf=RandomForestClassifier(random_state=42)<br><br>y_probas_predict=cross_val_predict(forest_clf,X_train,y_train_5,cv=5,method='predict_proba')</pre> <pre>from sklearn.metrics import roc_curve<br><br>fpr,tpr,threshold=roc_curve(y_train_5,y_scores)<br><br><br>y_scores_forest=y_probas_predict[:,1] #For probability of positive outcomes<br><br>fpr_forest,tpr_forest,threshold=roc_curve(y_train_5,y_scores_forest)</pre> <pre>plt.plot(fpr,tpr,"b:",label="SGD")<br>plt.plot(fpr_forest,tpr_forest,"r-",label="Random Forest")<br>plt.plot([0,1], [0, 1], 'k--') <br>plt.grid(visible=True)<br>plt.xlabel("False Positive Rate")<br>plt.ylabel("True Positive Rate")<br><br>plt.legend()    <br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*o4_qlJF4Jw0fsmafTrCaeg.png"><figcaption>The ROC curve for both Models</figcaption></figure> <p>From this, we can conclude that the Random Forest Classifier looks much better than the SGD Classifier: the area under the curve is higher for Random Forest than it is for SGD.</p> <pre>roc_auc_score(y_train_5,y_scores_forest)<br><br># Output: 0.998402186461512</pre> <p>This further justifies our remarks as the ROC-AUC for SGD is 0.965 compared to 0.998 for Random Forest.</p> <h3>Reflection</h3> <p>With the end of Binary Classification, I will now be moving on to Multiclass Classification.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0fa9007b125b" width="1" height="1" alt=""></p> </body></html>