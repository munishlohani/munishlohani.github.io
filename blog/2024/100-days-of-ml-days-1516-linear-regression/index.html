<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oi1uLRrshHT6n_Sa"><figcaption>Photo by <a href="https://unsplash.com/@afgprogrammer?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Mohammad Rahmani</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>Introduction</h3> <p>Previously, we had been working with models like a black box — we knew what they were used for, but we didn’t fully understand how they processed the data to make it useful. Now, let’s dive deeper into some algorithms and uncover their underlying mechanisms!</p> <p>Let’s start with <strong>Linear Regression</strong>.</p> <p>Simply put, linear regression models the relationship between a dependent variable (or target vector) and one or more independent variables (or feature vectors).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/192/1*mQmaO5c7sob72GrrMLAfBQ.png"></figure> <p>Here, we have a collection of data consisting of feature vectors and their corresponding target vector. We can build a model as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/588/1*TLs0PYGjUE-dAMyG50TCDw.png"><figcaption>A Linear Regression Model</figcaption></figure> <p><strong>A Linear Regression Model</strong>, where:</p> <ul> <li> <strong>w</strong> is the feature weight</li> <li> <strong>b</strong> is the bias</li> </ul> <p>In machine learning, vectors are typically represented as column vectors. So, if <strong>w</strong> and <strong>x</strong> are column vectors, the prediction is given by:<br><strong>y = transpose(w) * x</strong>, where <strong>transpose(w)</strong> simply means converting the column vector into a row vector.</p> <p><strong>How do we train it?</strong><br>To train a regression model, we adjust its parameters (weights) to minimize the error. In mathematics, the expression we aim to minimize (or maximize) is called the <strong>objective function</strong>.</p> <p>For a linear regression hypothesis <strong>h</strong> on a training set <strong>X</strong>, the <strong>Mean Squared Error (MSE)</strong> is calculated as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/347/1*GX7-0xCQ58ttAvFeK1tl6w.png"><figcaption>MSE for Linear Regression</figcaption></figure> <p>Here, this expression is the loss function. It’s a measure of penalty for misclassification. MSE ensures that larger errors are penalized heavily.</p> <h3>Assumptions</h3> <p>For a linear regression, there are certain things that we usually consider:</p> <p>· <strong>Linearity</strong>: The relationship between dependent variables and independent variables is linear.</p> <p>· <strong>Independence</strong>: The observations are independent of each other.</p> <p>· <strong>Normality</strong>: The errors are distributed normally i.e. they form a bell curve.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=acf5f19228d7" width="1" height="1" alt=""></p> </body></html>