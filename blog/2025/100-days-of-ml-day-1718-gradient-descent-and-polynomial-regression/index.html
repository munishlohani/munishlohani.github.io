<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Gradient Descent</h3> <p>Gradient Descent is an optimization algorithm that iteratively adjusts parameters to minimize the cost function. It measures the local gradient of the cost function with respect to the parameters and moves in the direction of the negative gradient. Once the gradient becomes zero, the algorithm has reached the minimum point. In each iteration, the parameters are updated, and this process continues until the minimum point of the loss function is found.</p> <p>An important hyperparameter in gradient descent is the <strong>learning rate</strong>, which determines the size of the step taken after each iteration. If the learning rate is too small, it may take many iterations to converge to the minimum. Conversely, if the learning rate is too large, the algorithm might overshoot the minimum or fail to converge altogether.</p> <p>In the case of the Mean Squared Error (MSE) cost function for linear regression, the function is <strong>convex</strong>, meaning it has no local minima — only a single global minimum.</p> <h3>Batch Gradient Descent</h3> <p>In <strong>Batch Gradient Descent</strong>, the entire dataset is used to compute the gradient of the cost function at each iteration. For linear regression, the cost function is:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/322/1*yZhgjX3q6KVV0e1DOYFCAw.png"></figure> <p>The gradient of the cost function with respect to each parameter is calculated as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/262/1*e97vaJdSftQK-AMP3H2bug.png"></figure> <p>Finally, in order to update the parameter, we use:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/271/1*U3uu0I9s5e339vHeN9RqRA.png"></figure> <h3>Stochastic Gradient Descent</h3> <p>The main drawback of Batch Gradient Descent is that it uses the entire training set to compute the gradient at each step, which can be computationally expensive for large datasets. <strong>Stochastic Gradient Descent (SGD)</strong>, on the other hand, uses a single random training example to compute the gradient and update the parameters. This significantly reduces computational complexity.</p> <p>However, unlike Batch Gradient Descent, where the cost function decreases smoothly, Stochastic Gradient Descent often shows fluctuations, with the cost sometimes increasing or decreasing unpredictably before converging near the minimum.</p> <h3><strong>Polynomial Regression</strong></h3> <p>Not all relationships between features and the target variable are linear. What if the data follows a quadratic or cubic relationship? Surprisingly, we can still use a linear model to fit non-linear data by transforming the features.</p> <p>For example, consider a quadratic relationship. First, let’s generate some random data:</p> <pre>import numpy as np<br><br>import matplotlib.pyplot as plt<br>m = 100<br>X = 6 * np.random.rand(m, 1) - 3<br>y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)</pre> <pre>plt.scatter(X,y)<br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/543/1*Y6w-5owrPqeLc9ATaI2F1A.png"></figure> <pre><br>from sklearn.preprocessing import PolynomialFeatures<br><br>poly=PolynomialFeatures(degree=2,include_bias=False)<br>X_poly=poly.fit_transform(X)</pre> <p>This allows us to add a second feature to our data, degree 2. Finally, let’s again use Linear Regression</p> <pre>from sklearn.linear_model import LinearRegression<br><br>lin_reg=LinearRegression()<br>model=lin_reg.fit(X_poly,y)<br><br>pred=model.predict(X_poly)</pre> <pre>plt.scatter(X,y, color='blue', s=15)<br>X_range = np.linspace(min(X), max(X), 100).reshape(-1, 1)<br>X_poly_range = poly.transform(X_range)<br>y_range_pred = model.predict(X_poly_range)<br>plt.plot(X_range, y_range_pred, color='red', label='Polynomial regression line')<br>plt.legend()<br>plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/543/1*To9SirGWtrnfiT_aCRi43g.png"></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=13ee0bb0755f" width="1" height="1" alt=""></p> </body></html>