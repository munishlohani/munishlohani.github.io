<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZZkO2o6aTGZCUDS8"><figcaption>Photo by <a href="https://unsplash.com/@ikukevk?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Kevin Ku</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3><strong>Introduction</strong></h3> <p>A couple of months ago, I was just surfing Google, trying to find an area of study that could engage my mind. Previously, I had tried my hand at designing and had a good grasp of it. But I wanted to try something different, something new, and something that would captivate my mind. This was when I came across Data Science. At first, I thought it was simple — a field that simply deals with data. And I was partly right, but mostly wrong. It was, in fact, a lot more than just working with data. This was when I decided to give it a try, learn Python, and do some actual data science. I felt like a hardcore programmer for a couple of weeks. A month has passed, and now I’m here writing this blog about a simple emotion classifier that I made using Python and its endless resources.</p> <h3><strong>The Setup</strong></h3> <p>Before going to the data training part, lets just go through the packages and the environment I used.</p> <pre>import numpy as np<br>import pandas as pd<br>import re,string<br>import matplotlib.pyplot as plt<br>from nltk.corpus import stopwords<br>from nltk.stem import PorterStemmer<br>from sklearn.feature_extraction.text import TfidfVectorizer<br>from sklearn.pipeline import Pipeline<br>from sklearn.metrics import classification_report, confusion_matrix<br>from sklearn.model_selection import train_test_split<br>from sklearn.feature_selection import SelectKBest, chi2<br>from sklearn.metrics import accuracy_score<br>from sklearn.metrics import ConfusionMatrixDisplay<br>import pickle<br>import nltk<br>nltk.download('stopwords')<br>%matplotlib inline</pre> <h3><strong>Program</strong></h3> <p>Using pandas, the first thing is to open the <a href="https://www.kaggle.com/datasets/anjaneyatripathi/emotion-classification-nlp?select=emotion-labels-test.csv" rel="external nofollow noopener" target="_blank">dataset</a> and clean the data if necessary. Since the dataset that I used did not contained any null value, I proceeded into text cleaning.</p> <pre># Opening the dataset<br>df=pd.open_csv('Emotion_classify_Data.csv')<br>df.head() # To check the first 5 values in the dataset</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/471/1*xSFJnqegGqadFpdQlCk0aA.png"></figure> <h3>Text-Cleaning</h3> <pre>stemmer=PorterStemmer()<br>stopwords= stopwords.words('english')<br>df['cleaned']=list(filter(lambda x:[i for i in re.sub("[^a-zA-Z]"," ",x).split() if i not in stopwords],df['Comment']))<br>df['cleaned']=df['cleaned'].apply(lambda x: " ".join([stemmer.stem(i) for i in x.lower().split()]))<br>df</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*PyPCW6jUjepEiFqBGZkZ2w.png"></figure> <p>Here, I used a stemmer to reduce variation for improving text analysis and, thus, enhance informational retrieval. To understand the stemmer, let’s consider the following example: ‘I love computing’ is changed into ‘I love comput’ after stemming.</p> <p>Similarly, stopwords are used to remove some recurring words that carry no semantic meaning for text classification. The stopwords that can be filtered out during this process include ‘I’, ‘me’, ‘they’, ‘them’, ‘which’, ‘who’, ‘while’…</p> <p>After defining variables for stemming and stopwords, I began the text classification by introducing a new column in our dataset, i.e., df[‘cleaned’]. Using a lambda function and regex, I initially filtered out words that do not belong to the category ‘aA-zZ’ and removed stopwords present in the data. This includes removing ‘$’, ‘#’, ‘%’, ‘&amp;’, etc. The data was then stored in the new column. In the final step of text cleaning, I stemmed the data and stored it in the same column, replacing the previously stored data.</p> <h3>Training the model</h3> <p>Now comes the most interesting part in the whole project: Training our text-classifier. For this project we are using sci-kit learn to do the job for us.</p> <p>At first let’s import some packages that we had not done previously.</p> <pre>from sklearn.linear_model import LogisticRegression</pre> <p>Then, let’s use a vectorizer to convert our text data into a matrix of numerical form. For this project, we are utilizing the TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer. Similarly, to select the top-most features, we can employ the Chi-square test. Additionally, for the classifier, I have used Logistic Regression.</p> <p>With these components, we can create a pipeline that preprocesses the data using the vectorizer, then selects the top features using Chi-square, and finally uses Logistic Regression to predict texts.</p> <pre>vectorizer=TfidfVectorizer(stop_words='english')<br>x=df['cleaned']<br>y=df['Emotion']<br><br>x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15)<br><br><br>#pipeline<br>pipeline=Pipeline([('vect',vectorizer),<br>                    ('chi', SelectKBest(chi2, k=2000)),<br>                ('clf',LogisticRegression(random_state=0))<br>])<br><br>model=pipeline.fit(x_train,y_train)</pre> <p>This completes the modelling part for our text-classifer model. Now, lets test the model to check its accuracy as well as do some prediction.</p> <pre>predict_emotion=model.predict(x_test)<br>print('Accuracy score', accuracy_score(y_test,predict_emotion))</pre> <p>Here, we make a prediction using <em>model.predict (x_test)</em> that uses the text from our test data to create a prediction. In order to check the accuracy of our model, we compare our predicted result with our actual result from the test dataset. This is done using <em>accuracy_score(y_test,predict_emotion)</em></p> <figure><img alt="The text presents the accuracy score of our model that is 0.919 or 91.9%" src="https://cdn-images-1.medium.com/max/398/1*SLnmA3PycEAGOe7sAVP5hA.png"><figcaption>Accuracy Score of the model</figcaption></figure> <p>Now that we know the accuracy of our model, let’s try inputting our own text!</p> <pre>text=input('Enter a type of text: ')<br>pred={'predicted text':[text]}<br>dataframe=pd.DataFrame(pred)<br>prediction=model.predict(dataframe['predicted text'])<br>print('predicted class= ',prediction[0])</pre> <p>Here, we define <em>text </em>variable to store our input data. Then, we make a dictioanry where we store our text in the key, <em>predicted text. </em>Finally, we convert the dictionary into a data frame uing pandas and predict our data using <em>model.predict.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/653/1*JxWH1jWB84RuK5vrIN8IdA.png"><figcaption>The output of our model</figcaption></figure> <h3>Assessing the performence of our model</h3> <p>Assessing is another important step in machine learning. This will help us evaluate our model as well as identify issues in it. In order to access our model, I have used confusion matrix. A confusion matrix helps us identify how well our model is predicting individual classes.</p> <pre>ytest=np.array(y_test)<br>print(classification_report(model.predict(x_test),ytest))<br>print(confusion_matrix(model.predict(x_test),y_test))</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/652/1*mokjHnnBcdYoSN1rViwAnQ.png"></figure> <p>These data evaluates the overall performence of our text classifier. In order to view graphical representation, we can plot the confusion matrix.</p> <pre>cm = confusion_matrix(model.predict(x_test),ytest)<br><br>fig, ax = plt.subplots(figsize=(8,8), dpi=100)<br>class_names=['anger','fear','joy']<br><br>display = ConfusionMatrixDisplay(cm, display_labels=class_names)<br><br>ax.set(title='Confusion Matrix for Emotion Detection Model')<br><br><br>display.plot(ax=ax);</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/683/1*nzRfjNzUMH0VlrIkuidlXg.png"></figure> <p>Here, the total number of anger in our test data set is 294. However, our model has classified only 275 of them as anger and rest as either joy or fear. This explains the accuracy of our model and helps us enhance the overall performance.</p> <h3>Conclusion</h3> <p>This ends our project of creating a text-classification model that can identify three major emotions: Fear, Joy and Anger.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dd8c77fade9a" width="1" height="1" alt=""></p> </body></html>