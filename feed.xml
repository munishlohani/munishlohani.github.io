<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://munishlohani.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://munishlohani.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-29T14:06:22+00:00</updated><id>https://munishlohani.github.io/feed.xml</id><title type="html">Munish Lohani</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">100 Days of ML: Day 17–18, Gradient Descent and Polynomial Regression</title><link href="https://munishlohani.github.io/blog/2025/100-days-of-ml-day-1718-gradient-descent-and-polynomial-regression/" rel="alternate" type="text/html" title="100 Days of ML: Day 17–18, Gradient Descent and Polynomial Regression"/><published>2025-01-07T12:05:34+00:00</published><updated>2025-01-07T12:05:34+00:00</updated><id>https://munishlohani.github.io/blog/2025/100-days-of-ml-day-1718-gradient-descent-and-polynomial-regression</id><content type="html" xml:base="https://munishlohani.github.io/blog/2025/100-days-of-ml-day-1718-gradient-descent-and-polynomial-regression/"><![CDATA[<h3>Gradient Descent</h3> <p>Gradient Descent is an optimization algorithm that iteratively adjusts parameters to minimize the cost function. It measures the local gradient of the cost function with respect to the parameters and moves in the direction of the negative gradient. Once the gradient becomes zero, the algorithm has reached the minimum point. In each iteration, the parameters are updated, and this process continues until the minimum point of the loss function is found.</p> <p>An important hyperparameter in gradient descent is the <strong>learning rate</strong>, which determines the size of the step taken after each iteration. If the learning rate is too small, it may take many iterations to converge to the minimum. Conversely, if the learning rate is too large, the algorithm might overshoot the minimum or fail to converge altogether.</p> <p>In the case of the Mean Squared Error (MSE) cost function for linear regression, the function is <strong>convex</strong>, meaning it has no local minima — only a single global minimum.</p> <h3>Batch Gradient Descent</h3> <p>In <strong>Batch Gradient Descent</strong>, the entire dataset is used to compute the gradient of the cost function at each iteration. For linear regression, the cost function is:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/322/1*yZhgjX3q6KVV0e1DOYFCAw.png"/></figure> <p>The gradient of the cost function with respect to each parameter is calculated as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/262/1*e97vaJdSftQK-AMP3H2bug.png"/></figure> <p>Finally, in order to update the parameter, we use:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/271/1*U3uu0I9s5e339vHeN9RqRA.png"/></figure> <h3>Stochastic Gradient Descent</h3> <p>The main drawback of Batch Gradient Descent is that it uses the entire training set to compute the gradient at each step, which can be computationally expensive for large datasets. <strong>Stochastic Gradient Descent (SGD)</strong>, on the other hand, uses a single random training example to compute the gradient and update the parameters. This significantly reduces computational complexity.</p> <p>However, unlike Batch Gradient Descent, where the cost function decreases smoothly, Stochastic Gradient Descent often shows fluctuations, with the cost sometimes increasing or decreasing unpredictably before converging near the minimum.</p> <h3><strong>Polynomial Regression</strong></h3> <p>Not all relationships between features and the target variable are linear. What if the data follows a quadratic or cubic relationship? Surprisingly, we can still use a linear model to fit non-linear data by transforming the features.</p> <p>For example, consider a quadratic relationship. First, let’s generate some random data:</p> <pre>import numpy as np<br /><br />import matplotlib.pyplot as plt<br />m = 100<br />X = 6 * np.random.rand(m, 1) - 3<br />y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)</pre> <pre>plt.scatter(X,y)<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/543/1*Y6w-5owrPqeLc9ATaI2F1A.png"/></figure> <pre><br />from sklearn.preprocessing import PolynomialFeatures<br /><br />poly=PolynomialFeatures(degree=2,include_bias=False)<br />X_poly=poly.fit_transform(X)</pre> <p>This allows us to add a second feature to our data, degree 2. Finally, let’s again use Linear Regression</p> <pre>from sklearn.linear_model import LinearRegression<br /><br />lin_reg=LinearRegression()<br />model=lin_reg.fit(X_poly,y)<br /><br />pred=model.predict(X_poly)</pre> <pre>plt.scatter(X,y, color=&#39;blue&#39;, s=15)<br />X_range = np.linspace(min(X), max(X), 100).reshape(-1, 1)<br />X_poly_range = poly.transform(X_range)<br />y_range_pred = model.predict(X_poly_range)<br />plt.plot(X_range, y_range_pred, color=&#39;red&#39;, label=&#39;Polynomial regression line&#39;)<br />plt.legend()<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/543/1*To9SirGWtrnfiT_aCRi43g.png"/></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=13ee0bb0755f" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML: Days 15–16, Linear Regression</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-days-1516-linear-regression/" rel="alternate" type="text/html" title="100 Days of ML: Days 15–16, Linear Regression"/><published>2024-12-29T12:13:57+00:00</published><updated>2024-12-29T12:13:57+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-days-1516-linear-regression</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-days-1516-linear-regression/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oi1uLRrshHT6n_Sa"/><figcaption>Photo by <a href="https://unsplash.com/@afgprogrammer?utm_source=medium&amp;utm_medium=referral">Mohammad Rahmani</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>Introduction</h3> <p>Previously, we had been working with models like a black box — we knew what they were used for, but we didn’t fully understand how they processed the data to make it useful. Now, let’s dive deeper into some algorithms and uncover their underlying mechanisms!</p> <p>Let’s start with <strong>Linear Regression</strong>.</p> <p>Simply put, linear regression models the relationship between a dependent variable (or target vector) and one or more independent variables (or feature vectors).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/192/1*mQmaO5c7sob72GrrMLAfBQ.png"/></figure> <p>Here, we have a collection of data consisting of feature vectors and their corresponding target vector. We can build a model as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/588/1*TLs0PYGjUE-dAMyG50TCDw.png"/><figcaption>A Linear Regression Model</figcaption></figure> <p><strong>A Linear Regression Model</strong>, where:</p> <ul><li><strong>w</strong> is the feature weight</li><li><strong>b</strong> is the bias</li></ul> <p>In machine learning, vectors are typically represented as column vectors. So, if <strong>w</strong> and <strong>x</strong> are column vectors, the prediction is given by:<br/><strong>y = transpose(w) * x</strong>, where <strong>transpose(w)</strong> simply means converting the column vector into a row vector.</p> <p><strong>How do we train it?</strong><br/>To train a regression model, we adjust its parameters (weights) to minimize the error. In mathematics, the expression we aim to minimize (or maximize) is called the <strong>objective function</strong>.</p> <p>For a linear regression hypothesis <strong>h</strong> on a training set <strong>X</strong>, the <strong>Mean Squared Error (MSE)</strong> is calculated as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/347/1*GX7-0xCQ58ttAvFeK1tl6w.png"/><figcaption>MSE for Linear Regression</figcaption></figure> <p>Here, this expression is the loss function. It’s a measure of penalty for misclassification. MSE ensures that larger errors are penalized heavily.</p> <h3>Assumptions</h3> <p>For a linear regression, there are certain things that we usually consider:</p> <p>· <strong>Linearity</strong>: The relationship between dependent variables and independent variables is linear.</p> <p>· <strong>Independence</strong>: The observations are independent of each other.</p> <p>· <strong>Normality</strong>: The errors are distributed normally i.e. they form a bell curve.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=acf5f19228d7" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML, Day 13–14: Multiclass Classification</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-1314-multiclass-classification/" rel="alternate" type="text/html" title="100 Days of ML, Day 13–14: Multiclass Classification"/><published>2024-07-26T08:11:16+00:00</published><updated>2024-07-26T08:11:16+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-day-1314-multiclass-classification</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-1314-multiclass-classification/"><![CDATA[<p>A deep dive into multiclass classification using One-vs-Rest algorithm</p> <h3>Introduction</h3> <p>Previously, we worked on binary classification, using our MNIST dataset to distinguish 2 possible results: “5” or “not5”. Now, we will create a multiclass model to detect all ten digits.</p> <p>In simple words, Multiclass Classification involves more than 2 classes. A simple way to understand this process is by taking “number recognition” as an example where we classify input from 0–9.</p> <p>Some algorithms such as SGD classifiers, Random Forest Classifiers, and naïve Bayes are inherently designed to handle multiclass classification. On the other hand, algorithms like SVM or Logistic Regressions are strictly binary classifiers.</p> <p>Yet, we can use various techniques to perform a multiclass classification using multiple binary classifiers.</p> <h3>One-vs-Rest (OvR)</h3> <p>We create a separate binary classifier for each class in the One-vs-Rest strategy. This is like building “5” vs “not 5” classifiers for every digit. In this method, we run all classifiers on our instance and choose the class with the highest probability.</p> <h3>One-vs-One (OvO)</h3> <p>n the One-vs-One strategy, we train a binary classifier for every pair of digits, meaning for every N class, we have N×(N−1)/2N \times (N-1)/2N×(N−1)/2 classifiers. Then, based on the prediction count for each class, we see which class wins the most duels and declare it as our prediction. To understand better, let’s look at an example:</p> <p>Suppose we want to make a model to distinguish between red, blue, and green colors. For this, we create, (3 X 2)/2=3 classifiers:</p> <p>1. (red, blue)</p> <p>2. (red, green)</p> <p>3. (blue, green)</p> <p>Now, we take one color and then make predictions for each model. Suppose the first model predicts the color as ‘red’, the second one also predicts it as ‘red’, but the third model predicts it as ‘blue’. This gives red 2 points whereas blue 1 point. Based on this vote count, we expect the color to be red.</p> <h3>Training a Multiclass Classifier</h3> <pre>from sklearn.svm import SVC<br /><br /><br />svm_clf=SVC()<br />svm_clf.fit(X_train,y_train)<br /><br /></pre> <p>Firstly, let’s look at SVMs. Although SVMs do not natively handle multiclass classification, they can nonetheless be trained for this purpose. Sci-Kit learns itself to detect when to use the binary classification algorithm and when to use multiclass classification, automatically running OvR or OvO.</p> <p>Under the hood, Scikit-Learn uses the OvO algorithm for SVC as SVMs typically work better on this strategy due to its margin-based optimization.</p> <pre>svm_clf.predict(X_train[0].reshape(1,-1))<br /><br /><br /># Output: array([5], dtype=uint8) </pre> <pre>scores=svm_clf.decision_function(X_train[0].reshape(1,-1))<br />scores<br /><br /># output: array([[ 1.72501977,  2.72809088,  7.2510018 ,  8.3076379 , -0.31087254,<br /> #        9.3132482 ,  1.70975103,  2.76765202,  6.23049537,  4.84771048]])</pre> <p>The decision function gives scores for each class based on the 45 OvO model.</p> <p>However, we can also force Sci-Kit to learn to implement OvR or OvO algorithms.</p> <h4>One-vs-One</h4> <pre>from sklearn.multiclass import OneVsOneClassifier<br /><br />ovo_cl=OneVsOneClassifier(SVC())<br /><br />ovo_cl.fit(X_train,y_train)</pre> <pre>ovo_pred=ovo_cl.predict(X_train[0].reshape(1,-1))<br />ovo_scores=ovo_cl.decision_function(X_train[0].reshape(1,-1))<br />ovo_scores<br /><br /><br /><br /># Output: array([[ 2.72420789,  2.72909219,  7.25265966,  8.30764043, -0.31037527,<br />#         9.31302684,  0.70957317,  2.76678409,  6.22757724,  4.84005057]])</pre> <p>Here, the decision scores are the highest among the 45 models.</p> <h4>One-vs-Rest</h4> <pre>from sklearn.multiclass import OneVsRestClassifier<br /><br /><br />ovr_cl=OneVsRestClassifier(SVC())<br /><br />ovr_cl.fit(X_train,y_train)</pre> <pre>ovr_cl.predict(X_train[0].reshape(1,-1))<br /><br /><br /># output: array([5], dtype=uint8)</pre> <p>To get the exact number of estimators used to make a final prediction, we can use .estimators_ from sk-learn.</p> <pre>len(ovr_cl.estimators_)<br /><br /><br /><br /># Output: 10 (For OvR, there are as many models as there are classes. In our case, it&#39;s 10.)</pre> <p>By now, we have looked at an algorithm that does not natively handle multiclass classification. Now, let’s use an algorithm that can natively handle it. For now, let’s use SGDClassifier.</p> <pre>from sklearn.linear_model import SGDClassifier<br /><br />sgd_clf=SGDClassifier()<br />sgd_clf.fit(X_train,y_train)</pre> <pre>sgd_clf.predict([X_train[0]])<br /><br /><br /># Output: array([5], dtype=uint8)</pre> <pre>sgd_clf.decision_function([X_train[0]])<br /><br /><br /># output: array([[-21552.37213583, -41286.82560334,  -8164.86249129,<br /> #         1210.45049096, -25640.34563332,   2378.29018512,<br />  #      -19578.5361586 , -18033.75981984, -10719.81508551,<br />   #     -10666.77543211]]</pre> <p>The SGDClassifier looks much more convincing at predicting the numbers than SVC. However, there remains a doubt between 3 and 5.</p> <p>So, let’s evaluate our model.</p> <pre>from sklearn.model_selection import cross_val_score<br /><br /><br />cross_val_score(sgd_clf,X_train,y_train,cv=5,scoring=&quot;accuracy&quot;)<br /><br /><br /># Output: array([0.89516667, 0.85358333, 0.86325   , 0.88225   , 0.88166667])</pre> <p>The accuracy is generally above 85%. To improve on this, we can scale the values. This can be done via StandardScaler</p> <pre>from sklearn.preprocessing import StandardScaler<br /><br />scaler=StandardScaler()<br /><br />X_train_scaled=scaler.fit_transform(X_train.astype(np.float64))<br />cross_val_score(sgd_clf,X_train_scaled,y_train,cv=5,scoring=&quot;accuracy&quot;)<br /><br /><br /># Output: array([0.90266667, 0.8985    , 0.89975   , 0.89191667, 0.9045    ])</pre> <p>Now it looks much better!</p> <h3>Error Analysis</h3> <p>For now, let’s suppose we have found our best model and have successfully fine-tuned it. Now, let’s just look at ways to further enhance our model. One way of doing so is to analyze its errors.</p> <pre>from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix<br /><br />y_train_pred=cross_val_predict(sgd_clf,X_train_scaled,y_train,cv=3)<br />cm=confusion_matrix(y_train,y_train_pred)<br /><br />disp=ConfusionMatrixDisplay(confusion_matrix=cm)<br />disp.plot()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/516/1*-RaC_TOz7V_CqltQ7ZD_0g.png"/><figcaption>Confusion Matrix</figcaption></figure> <p>From our confusion matrix, we can infer that our model is reasonably accurate enough for most of our data. However, 5 remains quite green as compared to others. This can be due to less amount of data for “5” or due to difficulty in predicting it. To verify, we can compare error rates.</p> <pre>row_sums=np.sum(cm,axis=1)<br />err_conf_mx=cm/row_sums<br /><br />np.fill_diagonal(err_conf_mx,0)<br />plt.matshow(err_conf_mx)<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/410/1*OxGARhWGk-QpMpptKVezDg.png"/><figcaption>Error Rate</figcaption></figure> <p>From the figure, the column for “8” is quite bright as compared to others, meaning many images get misclassified as 8, mostly 3 and 5</p> <p>This can be due to various reasons: A common one being that the images are transformed.</p> <h3>Multilabel Classification</h3> <p>Sometimes, we may need to deal with data with multiple labels. A common example can be a photo classifier where we need to classify images based on the person present.</p> <p>While MNIST data doesn&#39;t come with multilabel, we can nonetheless modify it to accommodate different labels.</p> <pre>y_train_large=(y_train&gt;=5)<br />y_train_even=(y_train%2==0)<br /><br />y_multilabel=np.c_[y_train_large,y_train_even]</pre> <p>This creates 2 labels for each digit: one to classify whether the digit is greater than 5 and the other to classify whether the digit is even.</p> <p>Now, let’s train it.</p> <pre>from sklearn.neighbors import KNeighborsClassifier<br /><br />knn_clf=KNeighborsClassifier()<br />knn_clf.fit(X_train,y_multilabel)</pre> <pre>knn_clf.predict([X_train[0]])<br /><br /><br /># Output: array([[ True, False]])</pre> <p>We know that our first digit is 5. Since it satisfies the condition, n≥5, the first label is true. For the second label, since (5%2 =/= 0) , it returns false</p> <p>Finally, let’s evaluate a model. To evaluate a model, we can use the same metrics as we did earlier. For multilabel, we will just be taking an average of the scores.</p> <pre>from sklearn.metrics import f1_score<br />f1_score(y_multilabel,y_train_knn,average=&quot;weighted&quot;)<br /><br />#Output: 0.9812770550130211</pre> <h3>Reflection</h3> <p>With this chapter, I learned a lot about classification, its types as well as metrics to evaluate our model. The next blog will focus on questions/ projects related to classification.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2c834b45a49c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML, Day:11–12, Classification- Precision/Recall Tradeoff &amp;amp; ROC Curve</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-day1112-classification-precisionrecall-tradeoff-roc-curve/" rel="alternate" type="text/html" title="100 Days of ML, Day:11–12, Classification- Precision/Recall Tradeoff &amp;amp; ROC Curve"/><published>2024-07-17T11:20:17+00:00</published><updated>2024-07-17T11:20:17+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-day1112-classification--precisionrecall-tradeoff--roc-curve</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-day1112-classification-precisionrecall-tradeoff-roc-curve/"><![CDATA[<p>A deep dive into more classification metrics: Precision/ Recall Tradeoff and ROC Curve!</p> <h3>Introduction</h3> <p>Previously, we discussed Precision, Recall, and F-1 Score. This blog continues from my previous one. If you haven’t checked it out yet, you can do so <a href="https://medium.com/@munish.lohani/100-days-of-ml-day-9-10-classification-precision-and-recall-27ace9b94dd1">here</a>. Now, let’s get into our main topic!</p> <h3>Precision/Recall Tradeoff</h3> <p>In classification, we cannot have both Precision and Recall equally high. Improving one decreases the other, known as the Precision/Recall Tradeoff. To understand this, let’s go back to our SGDClassifier. For each instance, our model computes a score based on its decision function. For now, let’s say if the score is greater than its threshold, the result is positive; if it’s less, negative.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qjq57VBNs4woHWlPgMEACg.png"/><figcaption>Precision/Recall Tradeoff</figcaption></figure> <p>For this example, suppose the threshold is in the center. Now, based on this information, let’s calculate our Precision and Recall. Out of 5 positive predictions, 4 of them successfully identified the digit as 5, whereas 1 prediction was wrong. This gives:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/305/1*w1MBQ5Oz0pEmj3_eDw7-Xw.png"/><figcaption>Precision and Recall Calculation</figcaption></figure> <p>Now, let’s slightly move our threshold to the left. This gives,</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fqL-wTkAesn0KKOQCYYt-w.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/336/1*MyZO_dQ6flym4qJUowfG1g.png"/></figure> <p>As we can see, the Precision went down whereas we have a perfect Recall.</p> <p>Scikit-learn doesn’t let us set the threshold directly but it does allow us to access the decision scores using decision_function</p> <pre>y_scores=sgd_clf.decision_function(X[0].reshape(1,-1))<br />threshold=0<br />y_score_predict=(y_scores&gt;threshold)<br />y_score_predict<br /><br /><br /># Output: array([ True])</pre> <pre>threshold=6700<br />y_score_predict=(y_scores&gt;threshold)<br />y_score_predict<br /><br /><br /># Output: array([False])</pre> <p>This shows that by changing the threshold, we decreased the Recall. But how do we know which threshold to use? For this, we can find out the decision scores of all the instances and then compute Precision &amp; Recall for all possible thresholds.</p> <pre>y_scores=cross_val_predict(sgd_clf,X_train,y_train_5,cv=5,method=&#39;decision_function&#39;)<br /><br />from sklearn.metrics import precision_recall_curve<br /><br />precision,recall,threshold=precision_recall_curve(y_train_5,y_scores)</pre> <pre>def plot_prec_recall_thrshld(precision,recall,threshold):<br />    plt.plot(threshold,precision[:-1],&quot;b--&quot;,label=&quot;Precision&quot;)<br />    plt.plot(threshold,recall[:-1],&quot;r&quot;,label=&quot;Recall&quot;)<br />    plt.legend()<br />    plt.xlabel(&quot;Threshold&quot;)<br />    plt.grid(visible=True)<br /><br /><br />plot_prec_recall_thrshld(precision,recall,threshold)<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*Yxfk6Q57bhArMLh5q9S95g.png"/><figcaption>Precision and Recall Comparision</figcaption></figure> <p>Another way to visualize this is by plotting Precision vs. Recall.</p> <pre>plt.plot(recall,precision,&quot;b-&quot;,label=&quot;Precision&quot;)<br />plt.xlabel(&quot;precision&quot;)<br />plt.ylabel(&quot;Recall&quot;)<br />plt.grid(visible=True)<br />plt.show()<br /><br /></pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*_K6_Rrur1pIx_77ik-KWKQ.png"/><figcaption>Precision Vs Recall</figcaption></figure> <h3>The ROC Curve</h3> <p>The Receiver Operating Characteristic Curve is another method used in binary classifiers. Instead of plotting the Precision vs. Recall curve, the ROC curve plots the True Positive Rate (another name for Recall) vs. False Positive Rate (1 — True Negative Rate). To understand better, let’s take the above example.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qjq57VBNs4woHWlPgMEACg.png"/></figure> <p>Here,</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/452/1*1qYhhcafMoMU4-VK-JFiJA.png"/></figure> <h4><strong>AUC</strong></h4> <p>The Area Under the ROC curve is a single scalar value that summarizes the classifier&#39;s overall performance. Ranging from 0–1, the higher the value, the better performing the model. Now that we know what a ROC curve is, let’s use it for our model.</p> <pre>from sklearn.metrics import roc_auc_score<br /><br />roc_auc_score(y_train_5,y_scores)<br /><br /><br /># Output: 0.9648211175804801</pre> <p>As a rule of thumb, we use the P-R curve whenever the positive class is rare or when we care more about False Positives. In our case, since the positive class (5) occurs much more rarely than the negative class (not 5), the PR curve is better suited.</p> <p>With this, let’s try this on a different model — a RandomForest!</p> <pre>from sklearn.ensemble import RandomForestClassifier<br /><br /><br />forest_clf=RandomForestClassifier(random_state=42)<br /><br />y_probas_predict=cross_val_predict(forest_clf,X_train,y_train_5,cv=5,method=&#39;predict_proba&#39;)</pre> <pre>from sklearn.metrics import roc_curve<br /><br />fpr,tpr,threshold=roc_curve(y_train_5,y_scores)<br /><br /><br />y_scores_forest=y_probas_predict[:,1] #For probability of positive outcomes<br /><br />fpr_forest,tpr_forest,threshold=roc_curve(y_train_5,y_scores_forest)</pre> <pre>plt.plot(fpr,tpr,&quot;b:&quot;,label=&quot;SGD&quot;)<br />plt.plot(fpr_forest,tpr_forest,&quot;r-&quot;,label=&quot;Random Forest&quot;)<br />plt.plot([0,1], [0, 1], &#39;k--&#39;) <br />plt.grid(visible=True)<br />plt.xlabel(&quot;False Positive Rate&quot;)<br />plt.ylabel(&quot;True Positive Rate&quot;)<br /><br />plt.legend()    <br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/567/1*o4_qlJF4Jw0fsmafTrCaeg.png"/><figcaption>The ROC curve for both Models</figcaption></figure> <p>From this, we can conclude that the Random Forest Classifier looks much better than the SGD Classifier: the area under the curve is higher for Random Forest than it is for SGD.</p> <pre>roc_auc_score(y_train_5,y_scores_forest)<br /><br /># Output: 0.998402186461512</pre> <p>This further justifies our remarks as the ROC-AUC for SGD is 0.965 compared to 0.998 for Random Forest.</p> <h3>Reflection</h3> <p>With the end of Binary Classification, I will now be moving on to Multiclass Classification.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0fa9007b125b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML, Day 9–10: Classification-Precision and Recall</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-910-classification-precision-and-recall/" rel="alternate" type="text/html" title="100 Days of ML, Day 9–10: Classification-Precision and Recall"/><published>2024-07-12T07:09:41+00:00</published><updated>2024-07-12T07:09:41+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-day-910-classification-precision-and-recall</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-910-classification-precision-and-recall/"><![CDATA[<p>A deep dive into performance metrics- Precision, Recall, and F-1 Score!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JWqaam7vreotjAQM"/><figcaption>Photo by <a href="https://unsplash.com/@dawson2406?utm_source=medium&amp;utm_medium=referral">Stephen Dawson</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Previously, we looked at confusion matrix and acuracy. Now, we will be looking at 3 other metrices that can be used to evaluate a classification model.</p> <h4>Introduction</h4> <p>Precision and Recall are two crucial metrics used in model evaluation in classification tasks especially when dealing with imbalanced datasets.</p> <h4>Precision</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/322/1*MsxGp4AV_ENOYEDi2diw6g.png"/><figcaption>Precision</figcaption></figure> <p>Precision, also known as Positive Predictive Value, measures the proportion of True Positives (TP) out of all positive predictions (TP + FP) made by the model. Higher precision indicates better model performance with a low false positive rate.</p> <h4>Recall</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/218/1*s1sdVlXYggchN4eZswXV5g.png"/><figcaption>Recall</figcaption></figure> <p>Recall, also known as Sensitivity, measures the proportion of True Positive (TP) predictions out of all actual positive instances. Higher recall indicates a lower false negative rate.</p> <p>Now that we have a basic idea of Precision and recall, let’s use them in our code.</p> <pre>from sklearn.metrics import precision_score,recall_score<br /><br />precision=precision_score(y_train_5,y_train_pred)<br />recall=recall_score(y_train_5,y_train_pred)<br /><br /><br /><br />print(f&#39;&#39;&#39;<br />      Precision:{precision}<br />      Recall:{recall}<br />      <br />      &#39;&#39;&#39;)<br /><br />#Output:  Precision:0.7547327860613168<br /> #       Recall:0.8310274857037447</pre> <p>Now, our model doesnt look as attractive as it did earler with accuracy.</p> <p>With 0.754 precision, it means that 75.4% of`5 predictions were true. Lokewise, Recall of 0.831 means that out of all the actual positive values, the model predicted 83.1% correctly.</p> <h4>What if?</h4> <p><strong>High Precision, low Recall</strong></p> <p>High Precision, low Recall happens when the model is too conservative in predicting the positive class. This means the proportion of false positives is lower than that of false negatives.</p> <p><strong>Low Precision, High Recall</strong></p> <p>Low Precision and High Recall happen when the model is too aggressive in predicting positive classes. This means, the model tries to capture all positive cases, which minimizes false negatives and increases false positives.</p> <h4>F1 Score</h4> <p>It is often convenient to combine precision and recall into a single metric. We call it F1 Score. The F1 Score is the <em>Harmonic Mean</em><strong><em> </em></strong>of precision and recall, providing a balance between the two.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/330/1*6xJDSHv1gRPQAz4mK7e5bw.png"/><figcaption>Harmonic Mean, F1 Score</figcaption></figure> <p>Unlike a regular mean that treats all value equally, the harmonic mean gives much more weight to low values. As a result, to obtain a high F1 Score, it is necessary for both recall and precision to be high.</p> <pre>from sklearn.metrics import f1_score<br /><br />f1_score=f1_score(y_train_5,y_train_pred)<br /><br />print(f1_score)<br /><br />#Output:  0.7910447761194029</pre> <p>The F1 score favors classifiers with balanced precision and recall. However, the desired metric depends on the specific case. For instance, for a kid-safe video classifier, high precision is preferred to avoid false positives, while for a fraud detection model, high recall is crucial to minimize false negatives.</p> <p>But the problem is we can’t have it both ways: increasing precision reduces recall and vice-versa.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=27ace9b94dd1" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML, Day 7–8: Classification</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-78-classification/" rel="alternate" type="text/html" title="100 Days of ML, Day 7–8: Classification"/><published>2024-07-07T13:30:09+00:00</published><updated>2024-07-07T13:30:09+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-day-78-classification</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-78-classification/"><![CDATA[<p>Exploring binary classification and some performance measures</p> <h3>Classification</h3> <p>Previously, while talking about supervised learning, I mentioned regression and classification as some of the most popular techniques. While my previous project (new one on the way!) focused on regression, this and a couple of blogs will solely focus on classification.</p> <h4>Introduction</h4> <p>Classification is a fundamental task in machine learning that involves predicting the category or class of our input based on its features. Simply put, it means giving labels to our input.</p> <h4>MNIST</h4> <p>As cited in the book, I will also be taking reference of the MNIST dataset. It is a set of 70,000 small images of handwritten digits. Each image is labelled with the digit it represents.</p> <pre>from sklearn.datasets import fetch_openml<br />import numpy as np<br /><br />mnist=fetch_openml(&#39;mnist_784&#39;,version=1)</pre> <p>n the dataset, using mnist.keys(), we can access the dictionary keys from our set. Among these keys, DESCR contains a description of the dataset; data consists of an array with one row per instance and one column per feature; and target contains labels.</p> <p>To store it into X and y:</p> <pre>image=mnist.data.to_numpy()<br />X,y=image,mnist[&#39;target&#39;]</pre> <p>If we are to count total features and labels of the set, we can use NumPy&#39;s .shape attribute.</p> <pre>X.shape<br />y.shape<br /><br /># when executed seperately, output is:<br /># (70000,784) for X and (70000,) for y</pre> <p>From this, we can infer that there are 70,000 data points and 784 features. Each feature is a representation of a pixel’s intensity. Since each image is 28x28 pixels, the total pixel count is 784.</p> <p>Now, let’s visualize some images:</p> <pre>plt.subplot(521)<br />plt.imshow(X[0].reshape(28,28),cmap=plt.cm.gray_r)<br />plt.subplot(522)<br />plt.imshow(X[1].reshape(28,28),cmap=plt.cm.gray_r)<br />plt.subplot(523)<br />plt.imshow(X[2].reshape(28,28),cmap=plt.cm.gray_r)<br />plt.subplot(524)<br />plt.imshow(X[3].reshape(28,28),cmap=plt.cm.gray_r)<br />plt.subplot(525)<br />plt.imshow(X[4].reshape(28,28),cmap=plt.cm.gray_r)<br />plt.subplot(526)<br />plt.imshow(X[5].reshape(28,28),cmap=plt.cm.gray_r)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/384/1*5mhCUiIUXXNwTOMRIj5jUg.png"/><figcaption>Output</figcaption></figure> <p>Now, let&#39;s look at y. Yes, it is a string. To convert it into an integer, we can use NumPy&#39;s .astype:</p> <pre>y=y.astype(np.uint8)</pre> <p>Finally, let’s split the data into a training set and a testing set. For now, we will separate 60,000 for training and the remaining 10,000 for testing.</p> <pre>X_train,X_test,y_train,y_test=X[:60000],X[60000:],y[:60000],y[60000:]</pre> <h4>Binary Classification</h4> <p>Binary classification involves two classes: either true or false. For now, let’s make a binary classifier that recognizes the number ‘5’.</p> <pre>y_train_5=(y_train==5)<br />y_test_5=(y_test==5)</pre> <p>Now, let’s pick a classifier to train it. For now, let’s use Stochastic Gradient Descent.</p> <pre>from sklearn.linear_model import SGDClassifier<br /><br />sgd_clf=SGDClassifier(random_state=42)<br />sgd_clf.fit(X_train,y_train_5)</pre> <p>Let’s test it using the digit we used earlier</p> <pre>sgd_clf.predict(X_train[0].reshape(1,-1))<br /><br />#Output: array([ True])</pre> <p>Finally, let’s evaluate our model.</p> <h4>Performance Measure</h4> <p>Evaluating a classification model is often trickier compared to regression ones. Hence, throughout this segment, we will look at different performance measures.</p> <p><strong>Using Cross-Validation for Accuracy</strong></p> <p>Before we run the code, let’s understand <strong>accuracy</strong>. Accuracy is a common performance metric for classification tasks when the dataset we are working on is balanced. It measures the proportion of correct predictions out of the total number of predictions made.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*QB6Xhu_CS8xoUVvQ6gSxFw.png"/><figcaption>Accuracy</figcaption></figure> <pre>from sklearn.model_selection import cross_val_score<br /><br /><br />cross_val_score(sgd_clf,X_train,y_train_5,cv=5,scoring=&quot;accuracy&quot;)<br /><br /><br />#Output: array([0.95466667, 0.96975   , 0.9635    , 0.96533333, 0.94841667])</pre> <p>Although the scores are above 94%, they do not reflect true accuracy since 5s make up only 10% of the total data. To understand this better, let’s look at a model that returns “not 5” for every data point.</p> <pre>from sklearn.base import BaseEstimator<br /><br />class Never5(BaseEstimator):<br />    def fit(self,X,y=None):<br />        return self<br />    def predict(self,X):<br />        return np.zeros((len(X),1),dtype=bool)</pre> <pre>never_5=Never5()<br />cross_val_score(never_5,X_train,y_train_5,cv=5,scoring=&quot;accuracy&quot;)<br /><br />#Output: array([0.91266667, 0.90866667, 0.9095    , 0.90883333, 0.90858333])</pre> <p>Like earlier, the accuracy is quite high. Due to this reason, accuracy is generally not preferred for classification tasks where the data is skewed.</p> <p><strong>Confusion Matrix</strong></p> <p>The confusion matrix is another performance measure used in classification. It is a table that allows visualization of the algorithm’s performance.</p> <p><strong>Components of a Confusion Matrix</strong></p> <p>For binary classification, the confusion matrix summarizes 4 results in a 2x2 matrix.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/582/1*ed6_D322FMKQdhRVMFlAYw.png"/><figcaption>Confusion Matrix</figcaption></figure> <p>Finally, let’s express this in code.</p> <pre>from sklearn.metrics import confusion_matrix<br />from sklearn.model_selection import cross_val_predict<br /><br /><br />y_train_pred=cross_val_predict(sgd_clf,X_train,y_train_5,cv=5)<br />confusion_matrix(y_train_5,y_train_pred)<br /><br />#Output: array([[53115,  1464],<br />  #            [  916,  4505]], dtype=int64)</pre> <p>We can also represent this graphically.</p> <pre>from sklearn.metrics import ConfusionMatrixDisplay<br /><br />_=ConfusionMatrixDisplay.from_predictions(y_train_5,y_train_pred)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*Ro_QONheDeLxKEsRW2j00g.png"/><figcaption>Confusion Matrix</figcaption></figure> <p>Each row of the confusion matrix contains the actual class while the columns consist of the predicted class. Here, the first row represents the actual not-5 (the false class): 53,115 of them were correctly classified as not-5 (true negative) whereas 1,464 were classified as 5 (false positive). Likewise, the second row contains the actual 5 class (the true class): 916 were classified as not-5 (false negative) whereas 4,505 were classified as 5 (true positive).</p> <h4>Reflection</h4> <p>The confusion matrix provides us with a lot of data that can be used for performance measures. However, sometimes, we may need a concise number. Thus, my next blog will primarily focus on such measures.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9aef15d9b95d" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of Machine Learning, Day 5,6: Project One</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-machine-learning-day-56-project-one/" rel="alternate" type="text/html" title="100 Days of Machine Learning, Day 5,6: Project One"/><published>2024-07-01T05:51:37+00:00</published><updated>2024-07-01T05:51:37+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-machine-learning-day-56-project-one</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-machine-learning-day-56-project-one/"><![CDATA[<p>A California House Price Predictor!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*psi79DaNUmdmU4__"/><figcaption>Photo by <a href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral">Markus Spiske</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>Project One: California Housing Prices Dataset</h3> <h4><strong>Introduction</strong></h4> <p>It is finally time to code! For my first project, I followed the book “Hands-On Machine Learning with Scikit-Learn and TensorFlow.” The dataset used here gives an idea about housing prices based on location, rooms, bedrooms, etc. In this project, I will perform a basic machine learning task. To begin with, let’s complete some tasks.</p> <h4><strong>The Big Picture</strong></h4> <p>Our dataset contains data metrics such as population, median income, median housing cost, etc., for different districts.</p> <h4><strong>Frame The Problem</strong></h4> <blockquote><strong>Pipelines</strong></blockquote> <blockquote>In simple words, a data pipeline is a sequence of data processing components. Pipelines are crucial in machine learning, especially when managing complex workflows. This involves steps designed to transform our raw data into a model that can solve problems.</blockquote> <p>The next step is to ensure whether there are any existing ways to solve this problem. Knowing this will give us performance insights on how to solve the problem. For this problem, let’s assume that previously, the work was done manually: a separate department for data collection, a separate one for complex calculations, etc.</p> <p>Based on the current solution, we get an idea that the error in prediction can be high. What if we don’t have price data for some regions, and based on our complex calculations, we must predict a certain price? But then, what if the actual price comes out 10% or 20% off? Well, it certainly affects our business. But with our model, we can predict prices based on various features. Now that we know what our problem is, let’s frame it:</p> <ul><li>Our Data is labelled- <strong>Supervised Learning</strong>.</li><li>We are predicting the price- It is a regression task, specifically <strong>multiple regression</strong>.</li><li>We do not have a continuous source of data and we do not need our model to adapt to constantly changing data- it is <strong>batch learning</strong>.</li></ul> <h4>Select Performance Measure</h4> <p>Now that we have framed the problem, let’s select a performance measure for our model. In most regression tasks, we use <strong>Root Mean Square Error (RMSE)</strong> to measure our model’s performance.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/553/1*C-Yz2i_a1C4-OnVkbj6ZXA.png"/><figcaption>RMSE</figcaption></figure> <p>Another performance measure is <strong>Mean Absolute Error.</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/223/1*2UxOtkGvNy8LzhC-hk-7JA.png"/><figcaption>Mean Absolute Error</figcaption></figure> <p>Both RMSE and MAE are ways to calculate distance between the actual value vector and the predicted value vector.</p> <p><strong>RMSE</strong></p> <p>RMSE is based on Euclidean Norm. This measures the “straight-line” distance between two points in the Euclidean Space.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/292/1*7AuGH-ojQorMTxQWhljP-Q.png"/><figcaption>Euclidean Norm</figcaption></figure> <p><strong>MAE</strong></p> <p>MAE is based on Manhattan Norm. This measures the average absolute distances between actual values and the predicted values.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/207/1*qMhvexxjK9FFK_ua_FdB4Q.png"/><figcaption>Manhattan Norm</figcaption></figure> <p><strong>Check Assumptions</strong></p> <p>Finally, it is good practice to check whether the model that our data gets fit into categorizes the prices into categories or simply uses the prices themselves. For example, if the model uses categories, our task can be taken as a classification project instead of regression — we should be categorizing the house (cheap, medium, expensive) rather than predicting the price. For this project, let’s say it is a regression task.</p> <p>For this project, let’s say it is a regression task.</p> <h4>The Project</h4> <p>first, let’s load our data set.</p> <pre>import pandas as pd<br />import numpy as np<br /><br />housing=pd.read_csv(&quot;E://books to read//100 Days of Machine Learning//Code//Project_1_Cali_House_Dataset//housing.csv&quot;)<br />housing.head()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yn5qYj8J5sN9WK4YNy2sbQ.png"/><figcaption>Output</figcaption></figure> <pre>housing.info()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/530/1*bw51p2qxnPTzC6vAo51bIw.png"/><figcaption>Output</figcaption></figure> <p>We see that total_bedrooms has some missing values. We will resolve this issue later. Here, we also find that every other feature is a number (float64) but ocean_proximity is an object.</p> <pre>housing[&quot;ocean_proximity&quot;].value_counts()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/313/1*EncIlOVvCDTNmolBlK8feA.png"/><figcaption>Output</figcaption></figure> <p>So, it seems that there are five categories of ocean_proximity. Since our ML model only understands numbers, we will later be converting it into one via OneHotEncoding. For now, let&#39;s visualize the data in the form of a histogram.</p> <p>For now, let’s visualize the data inform of a histogram.</p> <pre>import matplotlib.pyplot as plt<br />%matplotlib inline<br /><br />housing.hist(bins=50,figsize=(20,15))<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*B5MeG3iBC5qouYbdMsQvCw.png"/><figcaption>Output</figcaption></figure> <p>From the figure, we can infer that most of the data seem to be negatively tailed. This might lead to a problem later in our ML model. So, it’s better to transform it into a bell-shaped curve. The next thing is the scale. For example, it seems that median_income has been transformed.</p> <p>Now, let&#39;s work on our data. In order to train our model, it is necessary to divide it into a training set and a testing set. A general rule is to allow 20% of data as testing and the rest as training. But, before we proceed, let&#39;s work on median_income. We see that most of the incomes are clustered around 1.5 and 6. But, there are also some beyond 6. So, it&#39;s important for us to have enough instances for each stratum.</p> <pre>housing[&quot;income_cat&quot;]=pd.cut(housing[&#39;median_income&#39;],bins=[0.,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])<br />housing[&quot;income_cat&quot;].hist()<br />plt.plot()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/561/1*rd1688oCkulLAKGwC3e3Jw.png"/><figcaption>output</figcaption></figure> <p>Now, lets work on training and testing set.</p> <pre>from sklearn.model_selection import StratifiedShuffleSplit<br /><br />split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)<br />for train_index,test_index in split.split(housing,housing[&#39;income_cat&#39;]):<br />    strat_train=housing.loc[train_index]<br />    strat_test=housing.loc[test_index]</pre> <p>In order for the model to be effective and unbiased, it is necessary to have a good proportion of every category in our training set. While we could use sklearn&#39;s train_test_split to divide our data, a more effective way to divide it as per the income category is StratifiedShuffleSplit. Finally, let&#39;s get the data back into its original form.</p> <pre>for set_ in (strat_test,strat_train):<br />    set_.drop(&quot;income_cat&quot;,axis=1,inplace=True)</pre> <p>Until now, we’ve just went through the data without digging deep into it. But now, let’s explore the data further. One way of doing so is via visualizing.</p> <pre>copy_housing=strat_train.copy()<br />copy_housing.plot(kind=&quot;scatter&quot;,x=&quot;longitude&quot;,y=&quot;latitude&quot;,alpha=0.1)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/573/1*UODEQZxTelhuR7pkXWMJqQ.png"/><figcaption>Output</figcaption></figure> <p>Well, this definately looks like California with higher density zones. Now, let’s visualize the income.</p> <pre>copy_housing.plot(kind=&quot;scatter&quot;,x=&quot;latitude&quot;,y=&quot;longitude&quot;,s=copy_housing[&quot;population&quot;]/100,c=&quot;median_income&quot;,cmap=plt.get_cmap(&quot;jet&quot;),alpha=0.3)<br />plt.legend()<br />plt.show()</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/574/1*fgDp9BnsKSYkCL_UNmHhyg.png"/><figcaption>Output</figcaption></figure> <p>Well, another way of exploring data is through finding correlation.</p> <pre>#Corr<br />corr=copy_housing.drop(&quot;ocean_proximity&quot;,axis=&quot;columns&quot;).corr()<br /><br />from pandas.plotting import scatter_matrix<br /><br />attributes=[&#39;median_house_value&#39;,&#39;median_income&#39;,&#39;total_rooms&#39;,&#39;housing_median_age&#39;]<br />scatter_matrix(copy_housing[attributes],figsize=(22,18))</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uEfVR0zjNekIFJioDq3vTQ.png"/><figcaption>Output</figcaption></figure> <p>From this graph, we can find that median_income and median_house_value have a quite linear relationship, whereas others aren&#39;t so clear. Well now, let&#39;s experiment by joining some values.</p> <pre>copy_housing[&quot;rooms_per_household&quot;]=copy_housing[&quot;total_rooms&quot;]/copy_housing[&quot;households&quot;]<br />copy_housing[&quot;bedrooms_per_room&quot;]=copy_housing[&quot;total_bedrooms&quot;]/copy_housing[&quot;total_rooms&quot;]<br />copy_housing[&quot;population_per_household&quot;]=copy_housing[&quot;population&quot;]/copy_housing[&quot;households&quot;]<br /><br />corr=copy_housing.drop(&quot;ocean_proximity&quot;,axis=&quot;columns&quot;).corr()<br />corr[&quot;median_house_value&quot;].sort_values(ascending=False)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/487/1*HHJu2_vjQWteMsXVoL7lIw.png"/><figcaption>Output</figcaption></figure> <p>Well, joining the dataset indeed helped us as rooms_per_household has a high correlation value with median_house_value. Now that we have performed some feature engineering, let&#39;s prepare our dataset based on features and labels.</p> <pre>housing_features=strat_train.drop(&quot;median_house_value&quot;,axis=1)<br />housing_label=strat_train[&#39;median_house_value&#39;].copy()</pre> <p>Now, we have perhaps reached an important point in our project. It’s time to create a pipeline. Remember, we had certain data missing in total_bedrooms? Now, we will be fixing it via sklearn&#39;s SimpleImputer.</p> <p>A SimpleImputer is used in the preprocessing stage of data preparation to handle missing values in a dataset. The goal is to replace null or missing values so that they do not negatively affect the performance of machine learning models. SimpleImputer does this by imputing values based on one of several strategies: Mean, Median, High Frequency, or Constant. Next, we will also be fixing the problem of tail-heavy data via StandardScaler.</p> <pre>from sklearn.preprocessing import StandardScaler<br />from sklearn.pipeline import Pipeline<br />from sklearn.impute import SimpleImputer<br /><br />pipeline=Pipeline([<br />    (&quot;imputer&quot;,SimpleImputer(strategy=&quot;median&quot;)),<br />    (&quot;std_Scaler&quot;,StandardScaler())<br />])</pre> <p>Now that we’ve created a pipeline to fix our data issues, let’s create a ColumnTransformer to specify the transformation for different columns.</p> <pre>num_features=housing_features.drop(&quot;ocean_proximity&quot;,axis=1)<br />num_attrs=list(num_features)<br />cat_attrs=[&quot;ocean_proximity&quot;]<br />full_pipeline=ColumnTransformer([<br />    (&quot;num&quot;,pipeline,num_attrs),<br />    (&quot;cat&quot;,OneHotEncoder(),cat_attrs)<br />])<br /><br />housing_prepares=full_pipeline.fit_transform(housing_features)</pre> <p>This concludes our preprocessing. Firstly, we fixed the issue of tail-heavy data &amp; NaN data. Then, we converted categorical data into numerical data.</p> <p>Now, we will start building our models.</p> <p><strong>Linear Regression</strong></p> <pre>from sklearn.linear_model import LinearRegression<br /><br />lin_reg=LinearRegression()<br />lin_reg.fit(housing_prepares,housing_label)</pre> <pre>from sklearn.metrics import mean_squared_error<br /><br />predictions=lin_reg.predict(housing_prepares)<br />lin_mse=mean_squared_error(housing_label,predictions)<br />lin_rmse=np.sqrt(lin_mse)<br />lin_rmse</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/205/1*lsYpJgTFLTyx5o4CPzooOQ.png"/><figcaption>RMSE output</figcaption></figure> <p>The value is better than nothing, but it is not perfect- in fact quite error prone. Most of our housing prices ranges from $128,000 to $225,000. So, an error of around $68,000 is rather imperfect. This means, our model isn’t powerful enough to grasp the information from our features.</p> <p><strong>Decision Tree</strong></p> <pre>from sklearn.tree import DecisionTreeRegressor<br />tree_reg=DecisionTreeRegressor()<br />tree_reg.fit(housing_prepares,housing_label)</pre> <pre>tree_predictions=tree_reg.predict(housing_prepares)<br />tree_mse=mean_squared_error(housing_label,tree_predictions)<br />tree_rmse=np.sqrt(tree_mse)<br /><br />tree_rmse<br /><br />#Output:0.0<br /></pre> <p>Well, our model turns out to be error free. But here is a problem- it shouldn&#39;t actually be perfect. By this, we can imply that the model is overfitting. So, how do we solve it? One way is to use sklearn’s cross_val_score.</p> <p>cross_val_score divides the training set into K-subsets, called folds. It involves training the model on some data set and then validating it on the remaining subset.</p> <pre>from sklearn.model_selection import cross_val_score<br /><br />cross_val_scores=cross_val_score(tree_reg,housing_prepares,housing_label,scoring=&quot;neg_mean_squared_error&quot;,cv=10)<br />tree_rmse_scores=np.sqrt(-cross_val_scores)<br /><br />tree_rmse_scores</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/758/1*sWp2Vqa9Q7Q5lHiynMnbUg.png"/><figcaption>Output</figcaption></figure> <p>Then, we can aggregate the mean value for all these values using np.mean() which comes out 71857.76227885179.</p> <p>Well, the Decision Tree performs worse as compared to Linear Regression.</p> <p><strong>Random Forest Regressor</strong></p> <p>Finally, let’s use RandomForestRegressor. A RandomForestRegressor is an ensemble learning method that constructs numerous DecisionTrees during training and outputs mean result.</p> <pre>from sklearn.ensemble import RandomForestRegressor<br /><br />forest_reg=RandomForestRegressor()<br />forest_reg.fit(housing_prepares,housing_label)<br />forest_pred=forest_reg.predict(housing_prepares)<br />forest_mse=mean_squared_error(housing_label,forest_pred)<br />forest_rmse=np.sqrt(forest_mse)<br />forest_rmse<br /><br />#Output: 18694.91813388159</pre> <pre>forest_cross_val_scores=cross_val_score(forest_reg,housing_prepares,housing_label,scoring=&quot;neg_mean_squared_error&quot;,cv=10)<br />forest_rmse_scores=np.sqrt(-cross_val_scores)<br /><br />forest_rmse_scores<br /><br />#Output: 68321.7118618  </pre> <p>Well, RandomForest looks much more promising, but the model is still overfitting. One way to solve this problem is by fiddling with hyperparameters.</p> <p><strong>GridSearchCV</strong></p> <p>sklearn’s GridSearchCV can help us automate the process of hyperparameter tuning. It also evaluates the performance of our models using cross validation.</p> <pre>from sklearn.model_selection import GridSearchCV<br /><br />param_grid=[{&quot;n_estimators&quot;:[3,10,30,60],&quot;max_features&quot;:[2,4,6,8,10]},<br />{&quot;bootstrap&quot;:[False],&quot;n_estimators&quot;:[3,10],<br /> &quot;max_features&quot;:[2,3,4]}]<br /><br />forest_reg=RandomForestRegressor()<br />grid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring=&quot;neg_mean_squared_error&quot;,return_train_score=True)<br /><br />grid_search.fit(housing_prepares,housing_label)</pre> <pre>grid_search.best_params_<br /><br />#Output: {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 60}</pre> <p>The .best_params_ gives the most effective hyperparameters for our model.</p> <pre>final_model=grid_search.best_estimator_<br /><br />x_test=strat_test.drop(&quot;median_house_value&quot;,axis=1)<br />y_test=strat_test[&quot;median_house_value&quot;].copy()<br /><br />x_test[&quot;rooms_per_household&quot;]=x_test[&quot;total_rooms&quot;]/x_test[&quot;households&quot;]<br />x_test[&quot;bedrooms_per_room&quot;]=x_test[&quot;total_bedrooms&quot;]/x_test[&quot;total_rooms&quot;]<br />x_test[&quot;population_per_household&quot;]=x_test[&quot;population&quot;]/x_test[&quot;households&quot;]<br /><br />x_test_prepared=full_pipeline.transform(x_test)<br /><br />final_pred=final_model.predict(x_test_prepared)</pre> <pre>final_mse=mean_squared_error(y_test,final_pred)<br />final_rmse=np.sqrt(final_mse)<br /><br />final_rmse<br /><br />#Output: 47297.496275008285</pre> <p>The root-mean square for our final model is the best one among the model we tried earlier. But our task doesn&#39;t end here. In some cases, we might have to find the range of our generalized error to be convinced. This can be done by finding the confidence interval for RMSE.</p> <pre>from scipy import stats<br /><br />confidence=0.95<br />error=(final_pred-y_test)**2<br />ci=np.sqrt(stats.t.interval(confidence,len(error)-1,loc=error.mean(),scale=stats.sem(error)))<br /><br />interval=ci[1]-ci[0]<br />mean_rmse=(ci[0]+ci[1])/2<br />proportion=(ci[1]-ci[0])/mean_rmse<br /><br />print(ci)<br /><br /><br />#Output:[45333.94519671 49182.71770318]<br /></pre> <h4>Reflection</h4> <p>By the end of day 6, not only was I able to learn the concepts of many Machine Learning Techniques but was also able to implement it.</p> <p>While this project is based in the book itself, I will also be working on a separate project to avoid “tutorial hell”.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a77b9b0ed97e" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of ML Day 3–4: Batch V/S Online Learning &amp;amp; Instance Based V/S Model-Based Learning</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-34-batch-vs-online-learning-instance-based-vs-model-based-learning/" rel="alternate" type="text/html" title="100 Days of ML Day 3–4: Batch V/S Online Learning &amp;amp; Instance Based V/S Model-Based Learning"/><published>2024-06-16T12:37:59+00:00</published><updated>2024-06-16T12:37:59+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-ml-day-34-batch-vs-online-learning--instance-based-vs-model-based-learning</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-ml-day-34-batch-vs-online-learning-instance-based-vs-model-based-learning/"><![CDATA[<h4>More Machine Learning Techniques: From Batch Vs Online Learning to Instance Based Vs Model Based</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NGaS4prpc7yWbNa3"/><figcaption>Photo by <a href="https://unsplash.com/@ikukevk?utm_source=medium&amp;utm_medium=referral">Kevin Ku</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h4><strong>Introduction</strong></h4> <p>For 100 days, I have set a mission to learn fundamental machine learning. For <a href="https://medium.com/@munish.lohani/100-days-of-machine-learning-day-1-2-the-machine-learning-landscape-caf2d04ebb76">days 1–2</a>, I read about a couple of machine learning systems. Since I wasn’t done with the topic, I decided to read further on it, and here it is.</p> <h4><strong>Batch and Online Learning</strong></h4> <p><strong>Batch Learning</strong></p> <p>This type of machine learning process is incapable of learning incrementally. This means the model should be trained with all the data before being deployed. In such a system, the training process is done offline and then taken to production where no more training is done. But what if there is a change in scenario and we need to train again? In such a scenario, we would have to retrain the model from scratch. But worry not! This process can be automated.</p> <p><strong>Online Learning</strong></p> <p>Online learning is a process where we can train the model incrementally by feeding data instances sequentially or in batches. Such a system is really beneficial where a system constantly needs to receive data. One famous example of such learning is the stock market, where data changes very frequently.</p> <p>But this isn’t the only advantage of this system. It can also discard data once it has been trained. This is quite beneficial when the system lacks resources.</p> <p>Now that we know something about online learning, let’s go more into it. One of the parameters of this method is the learning rate (η). In simple words, the learning rate determines how quickly a model adapts to changes in data.</p> <p><strong>Higher Learning Rate</strong>: The higher the learning rate, the more quickly the system adapts to change, but at the expense of forgetting old data. This means the system is more sensitive to any noise.</p> <p><strong>Low Learning Rate</strong>: The lower the learning rate, the slower the system will adapt to change. But this also leads to more stability in learning as the system is less likely to be affected by noise.</p> <h4>Instance Based VS Model Based Learning</h4> <p>A way of categorizing machine learning systems is through how they generalize. Most machine learning models are about making predictions for examples they have never seen before. Having a good performance measure for training is good but not sufficient.</p> <p>There are mainly two ways to categorize ML models on how they generalize:</p> <p><strong>Instance-based learning</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1008/0*3bBdwg5DMvLyj154.png"/></figure> <p>In this method, the system makes predictions based on similarity. This means the model compares the new data with the data it has been trained on and then makes a prediction. This is quite similar to ‘learning by heart,’ where the machine learns examples by ‘heart’ and then uses a similarity measure to compare. This method is also known as memory-based learning or lazy learning. One such example is KNN (K-Nearest Neighbors).</p> <p><strong>KNN:</strong></p> <ul><li><strong>Store Training Data:</strong> Firstly, the algorithm stores all the training data</li><li><strong>Determining K- neighbors: </strong>In this step, we determine the number of nearest neighbors we would be selecting to make prediction for our test data. The final result will be heavily influenced by the K-neighbors.</li><li><strong>Compute distances: </strong>The algorithm computes the distance between our test data and all the training data. Based on the value of K, it selects the ‘K’ number of distances based on their value.</li><li><strong>Prediction:</strong></li></ul> <p>§ <strong>Classification: </strong>For classification, the algorithm looks at the classes of the K nearest neighbors. Then, based on the frequency of these classes, it predicts the class for our output. It selects the most frequent class as the output.</p> <p>§ <strong>Regression:</strong> For regression, the algorithm takes the average of the K nearest neighbors. This value is used as the prediction for our test data.</p> <p><strong>Model-based Learning</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/637/0*6reZgoc9Hilq943X.png"/></figure> <p>As mentioned in the title, model-based learning is about making an optimal model to generalize. In this method, an optimal model with parameters is created to make predictions. But the question is, how do we create the best model? This is done by tweaking our model parameters. But how? This is done via performance measures. One of the popular performance measures is the cost function. A cost function measures how bad a prediction is. It calculates the error between the predicted value and the true value. Using the cost function, the algorithm optimizes itself. It tries to lower the cost value by readjusting the parameters and repeating the steps until the cost function converges to the minimum value.</p> <p><strong>Reflection</strong></p> <p>For days 3–4, I briefly went through the concepts of some more machine learning systems. Based on this knowledge, the further blog will be about a project on these topics.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1386ca72c91f" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">100 Days of Machine Learning- Day 1–2, The Machine Learning Landscape</title><link href="https://munishlohani.github.io/blog/2024/100-days-of-machine-learning-day-12-the-machine-learning-landscape/" rel="alternate" type="text/html" title="100 Days of Machine Learning- Day 1–2, The Machine Learning Landscape"/><published>2024-06-13T07:00:40+00:00</published><updated>2024-06-13T07:00:40+00:00</updated><id>https://munishlohani.github.io/blog/2024/100-days-of-machine-learning--day-12-the-machine-learning-landscape</id><content type="html" xml:base="https://munishlohani.github.io/blog/2024/100-days-of-machine-learning-day-12-the-machine-learning-landscape/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*adaPEKJjuqaFVIG9"/><figcaption>Photo by <a href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral">h heyerlein</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h4><strong>The Beginning</strong></h4> <p>With the turn of the year, I told myself, “Not another year doing nothing!” I wanted to do something productive — learn something new, learn the hottest trend around. This is when I started exploring Machine Learning.</p> <h4><strong>ML</strong></h4> <p>Unlike a pure beginner, I was already familiar with certain ML concepts and Python concepts. But I was not satisfied. I wanted to explore the theory even more than I had before. I wanted to be a complete Machine-Learning Enthusiast. Well, I decided to start the trendy challenge with a twist — 100 days of Code. Oh wait, not code!</p> <h4><strong>100 Days of Machine Learning</strong></h4> <p>Throughout these 100 days, I decided I want to spend at least an hour learning Machine Learning concepts. I decided on using the famous <em>Hands-on </em><strong><em>Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</em></strong> by Aurélien Géron and <strong><em>The Hundred-Page Machine Learning Book</em> </strong>by Andry Burkov as my references (also ChatGPT for clarification).</p> <h3><strong>Day 1–2: The Machine Learning Landscape</strong></h3> <p>For my first two days, I wanted to learn more about Machine Learning in general as well as dive into some of its components.</p> <h4><strong>What is Machine Learning?</strong></h4> <p>The term “Machine Learning” has gained popularity in recent times. However, it is anything but new. Machine learning algorithms have been used for decades. But its popularity rose after the 1990s when the famous spam filter began to be widespread. So, what really is Machine Learning? Simply put, machine learning is the art of programming a computer to solve practical problems using data. If you want a more sophisticated answer, machine learning can be defined as a computer program where the machine learns from experience ‘E’ with respect to some task ‘T’ and a performance measure ‘P’.</p> <h4><strong>Why Use Machine Learning?</strong></h4> <p>Let’s go back a few decades, long before spam detection using ML was a thing. Now, let’s try making a spam detection algorithm. What would our approach be? Well, firstly, we would make an algorithm that would detect certain phrases like “amazing,” “4 you,” “free” as spam.</p> <p>But there is a massive problem. What if the scammers write “4 u” instead? Would we have to change the algorithm again? Alas, we would have to. But with machine learning, we wouldn’t. Instead, we would simply feed it new spam data and let the computer do its thing. Isn’t it so simple?</p> <p>Let’s take another example — a different one. Suppose we are trying to make a speech recognition algorithm. What’s our plan? Well, going traditional, it’s quite difficult. But with machine learning, it’s the opposite. We would first feed in the audio data with labels of words and train a model to identify these words.</p> <p>In a nutshell, <strong>Machine Learning </strong>helps solve:</p> <ul><li>Problems for which existing solution requires a lot of fine-tuning</li><li>Problems that do not yield positive result with a traditional approach</li><li>Problems whose environment change frequently</li></ul> <h3><strong>Types of Machine learning Systems</strong></h3> <p>Now that we know the basic use cases of Machine Learning and what it actually is, let’s dig deeper into its systems — let’s get the fun stuff going!</p> <h3><strong>Supervised Learning</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/132/1*85mbi_QiWr9Y62eYmptlwA.png"/><figcaption>Supervised Learning</figcaption></figure> <p>In supervised learning, the data set (training set) contains labels (or answers). Each element ‘x’ is called a feature vector. A feature vector is a vector where x^j for j=1…D, where j defines each dimension that describes the property of the example. For instance, if our x refers to a person, its dimensions x^j include weight, height, age, etc. A typical supervised learning task includes classification and regression.</p> <h4><strong>Classification</strong></h4> <p>Let’s take a spam detector, for example. In a spam detector, while we train the model, we feed in training data that includes emails as well as the answers (spam/ham). This helps the machine learn to classify new emails, i.e., email classification.</p> <h4><strong>Regression</strong></h4> <p>Regression includes predicting values based on a given set of values (predictors). For example, if we are to predict the price of a car, we would need its mileage, horsepower, age, etc. These are known as predictors. To train a model, we provide a set of training data that consists of all these predictors along with the price. This helps the machine predict new prices based on the predictors.</p> <figure><img alt="A graph showing linear regression" src="https://cdn-images-1.medium.com/max/494/0*ELMEKRlaGzWDfR7W.png"/><figcaption>Wikipedia: Linear Regression</figcaption></figure> <h4><strong>Regression-Classification</strong></h4> <p>Some regression algorithms can also be used for classification. For example, logistic regression gives values between 0–1. This can also be used for spam detection (e.g., 0.2 spam).</p> <h3>Unsupervised Learning</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/78/1*3326DGNlfSBgdKMSR65xuQ.png"/><figcaption>Unsupervised Learning</figcaption></figure> <p>In unsupervised learning, the model takes a collection of unlabeled data and returns an output that either transforms the feature vector into another vector or modifies it into other values that can be used to solve a practical problem.</p> <h4><strong>Clustering</strong></h4> <p>Clustering is an unsupervised learning process where we divide sets of data points based on their similarity. Data points within the same group are more similar to each other than those in other groups.</p> <p>So, why cluster data? Well, let’s take an example: Let’s say we run an e-commerce company and want to sell our new product — an expensive perfume. But we don’t know the targeted audience to cater our product via email. This is where clustering comes in. Suppose we have a dataset of customers based on their salary and spending score — higher scores mean they spend more. Now, we plot a graph based on these features.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/571/1*cgeD2cr5B_8aFDa3DcMzeg.png"/></figure> <p>From the graph, we get four clusters: high income-high score, high income-low score, low income-high score, and low income-low score. So, if we are to sell our expensive product, we must try selling it to high income-high score or low income-high score customers via some discounts. We might also try selling it to high income-low score customers since some may buy it with a little discount. But trying to sell it to low income-low score customers is certainly a bad idea.</p> <h4><strong>Dimensionality Reduction</strong></h4> <p>This is a technique that takes data with numerous features but returns output with a reduced number of features. This technique helps simplify models, reduce storage, and reduce processing time. One way to do so is to merge similar features into one. For example, if the battery life and the age of a laptop are strongly correlated, a feature extraction technique like Principal Component Analysis (PCA) might transform these two features into a new set of uncorrelated features. These new features represent the underlying patterns in the data, capturing the most important information from the original features. This transformation process is called <strong>feature extraction</strong></p> <h4><strong>Anomaly Detection</strong></h4> <p>In this technique, the system is shown mostly normal instances during training. So, when it experiences an abnormality with data, it flags it as an anomaly. One of the popular uses is fraud detection.</p> <h4><strong>Associative Learning Rule</strong></h4> <p>In this technique, we dig deeper into a large set of data to find interesting relations among the features. For example, let’s say we have a set of data containing a list of customers who buy potato chips. Upon further investigation of the data, we also find that these sets of customers tend to buy Coca-Cola. Thus, we may want to place both items together to maximize sales.</p> <h3>Semi-Supervised Learning</h3> <p>This is a combination of supervised learning and unsupervised learning. Here, some instances are labeled whereas most are not. So, the system itself identifies each label based on the clusters. A popular example of semi-supervised learning is Google Photos. Once we upload all the photos to the service, it automatically recognizes the patterns of people and divides them into clusters (unsupervised). Now, once we provide labels for each person in a single photo, the algorithm automatically labels other photos of the same person.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/575/1*_0YH4foEmF9SPv2cr6LPag.png"/><figcaption>Semi Supervised Learning</figcaption></figure> <h3>Reinforcement Learning</h3> <p>Reinforcement learning is a sub-field of machine learning, a different beast. Here, the learning agent, called the agent, lives in an environment and can perceive the state of the environment. The machine can perform actions and in return get rewards (or penalties). Based on this, it must learn by itself the best action to get the maximum average result over time. This strategy is called the policy.</p> <p>Reinforcement learning is mostly used to solve a particular problem where the goal is long-term.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*RW8irvIvQV5CCWugXDa9Vg.png"/><figcaption>Reinforcement Learning</figcaption></figure> <h3>Reflection: Day 1–2</h3> <p>tarting this long journey is indeed challenging, but it is equally engaging and exciting. I will continue posting blogs throughout these hundred days — in <strong>batches</strong> if not every day.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=caf2d04ebb76" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">Making a Simple Text Classifier</title><link href="https://munishlohani.github.io/blog/2023/making-a-simple-text-classifier/" rel="alternate" type="text/html" title="Making a Simple Text Classifier"/><published>2023-11-06T16:59:43+00:00</published><updated>2023-11-06T16:59:43+00:00</updated><id>https://munishlohani.github.io/blog/2023/making-a-simple-text-classifier</id><content type="html" xml:base="https://munishlohani.github.io/blog/2023/making-a-simple-text-classifier/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZZkO2o6aTGZCUDS8"/><figcaption>Photo by <a href="https://unsplash.com/@ikukevk?utm_source=medium&amp;utm_medium=referral">Kevin Ku</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3><strong>Introduction</strong></h3> <p>A couple of months ago, I was just surfing Google, trying to find an area of study that could engage my mind. Previously, I had tried my hand at designing and had a good grasp of it. But I wanted to try something different, something new, and something that would captivate my mind. This was when I came across Data Science. At first, I thought it was simple — a field that simply deals with data. And I was partly right, but mostly wrong. It was, in fact, a lot more than just working with data. This was when I decided to give it a try, learn Python, and do some actual data science. I felt like a hardcore programmer for a couple of weeks. A month has passed, and now I’m here writing this blog about a simple emotion classifier that I made using Python and its endless resources.</p> <h3><strong>The Setup</strong></h3> <p>Before going to the data training part, lets just go through the packages and the environment I used.</p> <pre>import numpy as np<br />import pandas as pd<br />import re,string<br />import matplotlib.pyplot as plt<br />from nltk.corpus import stopwords<br />from nltk.stem import PorterStemmer<br />from sklearn.feature_extraction.text import TfidfVectorizer<br />from sklearn.pipeline import Pipeline<br />from sklearn.metrics import classification_report, confusion_matrix<br />from sklearn.model_selection import train_test_split<br />from sklearn.feature_selection import SelectKBest, chi2<br />from sklearn.metrics import accuracy_score<br />from sklearn.metrics import ConfusionMatrixDisplay<br />import pickle<br />import nltk<br />nltk.download(&#39;stopwords&#39;)<br />%matplotlib inline</pre> <h3><strong>Program</strong></h3> <p>Using pandas, the first thing is to open the <a href="https://www.kaggle.com/datasets/anjaneyatripathi/emotion-classification-nlp?select=emotion-labels-test.csv">dataset</a> and clean the data if necessary. Since the dataset that I used did not contained any null value, I proceeded into text cleaning.</p> <pre># Opening the dataset<br />df=pd.open_csv(&#39;Emotion_classify_Data.csv&#39;)<br />df.head() # To check the first 5 values in the dataset</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/471/1*xSFJnqegGqadFpdQlCk0aA.png"/></figure> <h3>Text-Cleaning</h3> <pre>stemmer=PorterStemmer()<br />stopwords= stopwords.words(&#39;english&#39;)<br />df[&#39;cleaned&#39;]=list(filter(lambda x:[i for i in re.sub(&quot;[^a-zA-Z]&quot;,&quot; &quot;,x).split() if i not in stopwords],df[&#39;Comment&#39;]))<br />df[&#39;cleaned&#39;]=df[&#39;cleaned&#39;].apply(lambda x: &quot; &quot;.join([stemmer.stem(i) for i in x.lower().split()]))<br />df</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*PyPCW6jUjepEiFqBGZkZ2w.png"/></figure> <p>Here, I used a stemmer to reduce variation for improving text analysis and, thus, enhance informational retrieval. To understand the stemmer, let’s consider the following example: ‘I love computing’ is changed into ‘I love comput’ after stemming.</p> <p>Similarly, stopwords are used to remove some recurring words that carry no semantic meaning for text classification. The stopwords that can be filtered out during this process include ‘I’, ‘me’, ‘they’, ‘them’, ‘which’, ‘who’, ‘while’…</p> <p>After defining variables for stemming and stopwords, I began the text classification by introducing a new column in our dataset, i.e., df[‘cleaned’]. Using a lambda function and regex, I initially filtered out words that do not belong to the category ‘aA-zZ’ and removed stopwords present in the data. This includes removing ‘$’, ‘#’, ‘%’, ‘&amp;’, etc. The data was then stored in the new column. In the final step of text cleaning, I stemmed the data and stored it in the same column, replacing the previously stored data.</p> <h3>Training the model</h3> <p>Now comes the most interesting part in the whole project: Training our text-classifier. For this project we are using sci-kit learn to do the job for us.</p> <p>At first let’s import some packages that we had not done previously.</p> <pre>from sklearn.linear_model import LogisticRegression</pre> <p>Then, let’s use a vectorizer to convert our text data into a matrix of numerical form. For this project, we are utilizing the TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer. Similarly, to select the top-most features, we can employ the Chi-square test. Additionally, for the classifier, I have used Logistic Regression.</p> <p>With these components, we can create a pipeline that preprocesses the data using the vectorizer, then selects the top features using Chi-square, and finally uses Logistic Regression to predict texts.</p> <pre>vectorizer=TfidfVectorizer(stop_words=&#39;english&#39;)<br />x=df[&#39;cleaned&#39;]<br />y=df[&#39;Emotion&#39;]<br /><br />x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15)<br /><br /><br />#pipeline<br />pipeline=Pipeline([(&#39;vect&#39;,vectorizer),<br />                    (&#39;chi&#39;, SelectKBest(chi2, k=2000)),<br />                (&#39;clf&#39;,LogisticRegression(random_state=0))<br />])<br /><br />model=pipeline.fit(x_train,y_train)</pre> <p>This completes the modelling part for our text-classifer model. Now, lets test the model to check its accuracy as well as do some prediction.</p> <pre>predict_emotion=model.predict(x_test)<br />print(&#39;Accuracy score&#39;, accuracy_score(y_test,predict_emotion))</pre> <p>Here, we make a prediction using <em>model.predict (x_test)</em> that uses the text from our test data to create a prediction. In order to check the accuracy of our model, we compare our predicted result with our actual result from the test dataset. This is done using <em>accuracy_score(y_test,predict_emotion)</em></p> <figure><img alt="The text presents the accuracy score of our model that is 0.919 or 91.9%" src="https://cdn-images-1.medium.com/max/398/1*SLnmA3PycEAGOe7sAVP5hA.png"/><figcaption>Accuracy Score of the model</figcaption></figure> <p>Now that we know the accuracy of our model, let’s try inputting our own text!</p> <pre>text=input(&#39;Enter a type of text: &#39;)<br />pred={&#39;predicted text&#39;:[text]}<br />dataframe=pd.DataFrame(pred)<br />prediction=model.predict(dataframe[&#39;predicted text&#39;])<br />print(&#39;predicted class= &#39;,prediction[0])</pre> <p>Here, we define <em>text </em>variable to store our input data. Then, we make a dictioanry where we store our text in the key, <em>predicted text. </em>Finally, we convert the dictionary into a data frame uing pandas and predict our data using <em>model.predict.</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/653/1*JxWH1jWB84RuK5vrIN8IdA.png"/><figcaption>The output of our model</figcaption></figure> <h3>Assessing the performence of our model</h3> <p>Assessing is another important step in machine learning. This will help us evaluate our model as well as identify issues in it. In order to access our model, I have used confusion matrix. A confusion matrix helps us identify how well our model is predicting individual classes.</p> <pre>ytest=np.array(y_test)<br />print(classification_report(model.predict(x_test),ytest))<br />print(confusion_matrix(model.predict(x_test),y_test))</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/652/1*mokjHnnBcdYoSN1rViwAnQ.png"/></figure> <p>These data evaluates the overall performence of our text classifier. In order to view graphical representation, we can plot the confusion matrix.</p> <pre>cm = confusion_matrix(model.predict(x_test),ytest)<br /><br />fig, ax = plt.subplots(figsize=(8,8), dpi=100)<br />class_names=[&#39;anger&#39;,&#39;fear&#39;,&#39;joy&#39;]<br /><br />display = ConfusionMatrixDisplay(cm, display_labels=class_names)<br /><br />ax.set(title=&#39;Confusion Matrix for Emotion Detection Model&#39;)<br /><br /><br />display.plot(ax=ax);</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/683/1*nzRfjNzUMH0VlrIkuidlXg.png"/></figure> <p>Here, the total number of anger in our test data set is 294. However, our model has classified only 275 of them as anger and rest as either joy or fear. This explains the accuracy of our model and helps us enhance the overall performance.</p> <h3>Conclusion</h3> <p>This ends our project of creating a text-classification model that can identify three major emotions: Fear, Joy and Anger.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dd8c77fade9a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>